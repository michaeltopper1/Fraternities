---
title: "proof_attempt"
output: 
  pdf_document:
    number_sections: true
    latex_engine: xelatex
indent: true
header-includes:
- \usepackage{amsfonts}
- \usepackage{amsthm}
- \usepackage{amsmath}
- \usepackage[english]{babel}
- \usepackage{bm}
- \usepackage{float}
- \usepackage[fontsize=12pt]{scrextend}
- \usepackage{graphicx}
- \usepackage{indentfirst}
- \usepackage[utf8]{inputenc}
- \usepackage{pdfpages}
- \usepackage[round,authoryear]{natbib}
- \usepackage{setspace}\doublespacing
- \usepackage{subfigure}
- \theoremstyle{definition}
- \newtheorem{definition}{Definition}[section]
- \newtheorem{assumption}{Assumption}[section]
- \newtheorem{theorem}{Theorem}[section]
- \newtheorem{corollary}{Corollary}[theorem]
- \newtheorem{lemma}[theorem]{Lemma}
- \newtheorem*{remark}{Remark}
- \newcommand{\magenta}[1]{\textcolor{magenta}{#1}}
- \newcommand{\indep}{\perp \!\!\! \perp}
- \floatplacement{figure}{H}

---
# Attempt at Proof

## Asymptotic Unbiasedness

I now focus on describing the conditions in which the treatment effect is asymptotically unbiased. The asymptotic bias of the treatment effect can be written as:

$$
\begin{aligned}
\lim_{T_0 \rightarrow \infty}\mathbb{E}[\hat{\tau}_{0,T_0+i}-\tau_{0,T_0+i}]&=\lim_{T_0 \rightarrow \infty}\mathbb{E}\left[Y_{0,T_0+i}(1)-\hat{Y}_{0,T_0+i}(0)-\left(Y_{0,T_0+i}(1)- Y_{0,T_0+i}(0)\right)\right]\\
&=\lim_{T_0 \rightarrow \infty}\mathbb{E}\left[Y_{0,T_0+i}(0)-\hat{Y}_{0,T_0+i}(0)\right]
\end{aligned}
$$


Prior to proving asymptotic unbiasedness of $\mathbb{E}\left[\alpha_{0,t}\right]$, some mechanisms must be introduced.

\theoremstyle{definition}
\begin{definition}{}
The Kullback-Leibler Divergence of the proposed distribution $Pr(y_{0,T_0+i}(0)|Y_0)$ with respect to the true distribution $f\left(y_{0,T_0+i}(0)\right)$ is defined as:
$$KL_f(\tilde{v})=\mathbb{E}\left(log\left(\frac{f\left(y_{0,T_0+i}(0)\right)}{Pr(y_{0,T_0+i}(0)|Y_0)}\right) \right)$$
\end{definition}

where $\tilde{v}$ denotes the parameters and $f$ the true distribution. The $KL_f(v) \ge 0$ with equality when the true data generating process is $Pr(y_{0,T_0+i}(0)|Y_0)$ [@gelman_bayesian_2014].

Define $v^0$ as the unique set of parameters such that:

$$v^0=argmin_{\tilde{v}} \mathbb{E}\left(log\left(\frac{f\left(y_{0,T_0+i}(0)\right)}{Pr(y_{0,T_0+i}(0)|Y_0)}\right)\right)$$

If the true data generating process is included in the model specification (i.e. $f\left(y_{0,T_0+i}(0)\right)=Pr(y_{0,T_0+i}(0)|Y_0)$), then $KL_f(v^0)=0$. Otherwise, $v^0$ is the value of the parameters that minimizes the distance between the model and the true data generating process with respect to the Kullback-Leibler Divergence.

The treatment effect can be written as $\mathbb{E}[\hat{\alpha}_{0,t}]=\mathbb{E}\left[y_{0,t}(1)-\hat{y}_{0,t}(0)\right]=\mathbb{E}\left[\alpha_{0,t}+y_{0,t}(0)-\hat{y}_{0,t}(0)\right]$. In order for $\mathbb{E}[\hat{\alpha}_{0,t}] \xrightarrow{p} \mathbb{E}[\alpha_{0,t}]$ as $T_{0}-1 \xrightarrow{}\infty$,  $\mathbb{E}\left[y_{0,t}(0)-\hat{y}_{0,t}(0)\right] \xrightarrow{p}0$. It is sufficient to show $Pr(\hat{y}_{0,T_0+i}(0)|Y_0) \xrightarrow{p} Pr({y}_{0,T_0+i}(0)|Y_0)$ as $T_{0}-1 \xrightarrow{}\infty$. In order to show this, it is sufficient to show that the posterior of the parameters converge to the true values (i.e. $Pr(v^0|Y_0) \xrightarrow{p} 1$).


\begin{theorem}[Asymptotic Unbiasedness]
\label{ass_unbias}
Assume that:

1) the model specification is the true data generating process.

2) $v$ is defined on a finite parameter space V and $Pr(v=v^0)>0$.

3) $\{y_{0,1}(0),...y_{0,T}(0)\}$ is a stationary ergodic process.

4) The poster distribution $Pr(v|Y_0)$ is unimodal.

If all the above assumptions are met, then $\mathbb{E}[\hat{\alpha}_{0,t}] \xrightarrow{p} \mathbb{E}[\alpha_{0,t}]$ as $T_{0}-1 \xrightarrow{p}\infty$.
\end{theorem}

I will prove this result in three steps. the first step is based off of @gelman_bayesian_2014 (appendix B).

\begin{proof}

Step 1: $Pr(v^0|Y_0) \xrightarrow{p} 1$

Define $v' \ne v^0$. Consider the following:


\begin{align}
log\left(\frac{Pr(v=v'|Y_0)}{Pr(v=v^0|Y_0)}\right)&=log\left(\frac{\pi(v=v')}{\pi(v=v^0)} \right)+log\left(\frac{Pr(Y_0|v)}{Pr(Y_0|v^0)}\right)  & \text{Bayes Rule}\\
&= log\left(\frac{\pi(v=v')}{\pi(v=v^0)}\right)+log\left(\frac{\Pi_{t=1}^{T_0-1}Pr(y_{0,t}(0)|v=v')}{\Pi_{t=1}^{T_0-1}Pr(y_{0,t}(0)|v=v^0)}\right) & \text{Model Specification}\\
&= log\left(\frac{\pi(v=v')}{\pi(v=v^0)}\right)+\sum_{t=1}^{T_0-1}log\left(\frac{Pr(y_{0,t}(0)|v=v')}{Pr(y_{0,t}(0)|v=v^0)}\right) & \text{log rules}\\
&= log\left(\frac{\pi(v=v')}{\pi(v=v^0)}\right)+\frac{T_0-1}{T_0-1}\sum_{t=1}^{T_0-1}log\left(\frac{Pr(y_{0,t}(0)|v=v')}{Pr(y_{0,t}(0)|v=v^0)}\right) & \text{multiply by 1}
\end{align}


Focus for a moment on the second term. Notice that as $T_0-1 \rightarrow \infty$:


\begin{align}
\frac{1}{T_0-1}\sum_{t=1}^{T_0-1}log\left(\frac{Pr(y_{0,t}(0)|v=v')}{Pr(y_{0,t}(0)|v=v^0)}\right) &\xrightarrow{} \mathbb{E}\left(log\left(\frac{Pr(y_{0,t}(0)|v=v')}{Pr(y_{0,t}(0)|v=v^0)}\right)\right) & \text{ergodic stationary}\\
&= \mathbb{E}\left(log\left(\frac{Pr(y_{0,t}(0)|v=v') f(y_{0,t}(0))}{Pr(y_{0,t}(0)|v=v^0)f(y_{0,t}(0))}\right)\right) & \text{multiply by 1}\\
&=KL_f(v^0)-KL_f(v')\\
&<0 
\end{align}

Combining this result and (10) yields:

$$
\begin{aligned}
log\left(\frac{Pr(v=v'|Y_0)}{Pr(v=v^0|Y_0)}\right) \xrightarrow{} -\infty 
\end{aligned}
$$

as $T_0-1 \rightarrow \infty$. Rearranging concludes $Pr(v=v'|Y_0) \rightarrow 0$ for all $v \ne v^0$. Therefore, $Pr(v=v^0|Y_0) \rightarrow 1$. Equivalently, $v \rightarrow v^0$ as $T_0-1 \xrightarrow{} \infty$.

Step 2: $Pr(\hat{y}_{0,T_0+i}(0)|Y_0) \xrightarrow{p} Pr({y}_{0,T_0+i}(0)|Y_0)$

Recall:

$$Pr(\hat{y}_{0,T_0+i}(0)|Y_0)=\sum_{v' \in V} Pr(y_{0,T_0+i}(0)|Y_0,v=v')Pr(v=v'|Y_0)$$


\magenta{I need help here. I'm not sure if I can pull the limit into the integral. I feel like I can't, but I also know this should be an asymptotic unbiased estimator from other papers.}


step 3: $\mathbb{E}[\hat{\alpha}_{0,t}] \xrightarrow{p} \mathbb{E}[\alpha_{0,t}]$ as $T_{0}-1 \xrightarrow{}\infty$

In order for $\mathbb{E}[\hat{\alpha}_{0,t}] \xrightarrow{p} \mathbb{E}[\alpha_{0,t}]$ as $T_{0}-1 \xrightarrow{}\infty$,  $\mathbb{E}\left[y_{0,T_0+i}(0)-\hat{y}_{0,T-0+i}(0)\right] \xrightarrow{p}0$

---Note for me---

Once I get step 2, I think I can say if $Pr(\hat{y}_{0,T_0+i}(0)|Y_0) \xrightarrow{p} Pr({y}_{0,T_0+i}(0)|Y_0)$, then $\mathbb{E}\left[y_{0,t}(0)-\hat{y}_{0,t}(0)|Y_0\right] \xrightarrow{p}0$ which then leads to $\mathbb{E}\left[\mathbb{E}\left[y_{0,t}(0)-\hat{y}_{0,t}(0)|Y_0\right]\right] \xrightarrow{p}0$.

---

\end{proof}

**Intuition** As the pre-treatment period gets larger and larger, the choice of priors will become less important. The data will converge to a point mass on the true parameter value, $v^0$. In turn, the mean of the poster predictive distribution will converge to the true value $y_{0,T_0+i}(0)$ leading to an asymptotic unbiased estimator of the treatment effect.

\begin{remark}
So long as the choice of priors assigns non-zero probability to the true parameter values $v^0$, then the choice of priors is irrelevant in the limit. However, this is not true for finite sample estimation. In application, the choice of priors can have massive effects on inference.
\end{remark}

\begin{remark}
This proof was shown using a finite parameter space. However, the assumption can be relaxed by assuming a compact set on the parameter space.
\magenta{Should I do it for continuous parameter space? I don't think it adds anything}
\end{remark}

\begin{remark}
This result provides assurance of unbiasedness in the limit. However, inference is not drawn from asymptotic results.
\end{remark}
#___________________________________________________________________________________#
