---
title: "related_work"
output: pdf_document
header-includes:
- \newcommand{\magenta}[1]{\textcolor{magenta}{#1}}
---

# Setup

## Potential Outcomes and Parameter of Interest

I define the treatment as an intervention or policy change.  Once treated, the unit remains treated indefinitely. The treatment status is known for all units in all time periods.

Let $\left(Y_{j,t}(0),Y_{j,t}(1)\right)$ represent potential outcomes in the presence and absence of a treatment with $t=1,...,T_{0}-1,T_0,T_{0}+1,...T$ and $j=0,1,...,J$. Denote the period of intervention as $T_0$. Define the treatment status as $D_j=\{0,1\}$, where a 1 indicates if the unit is treated in any period. I assume the potential outcomes are random.

Define $Y_{j,t}=(1-D_{j,t})Y_{j,t}(0)+D_{j,t}Y_{j,t}(1)$ where $D_{j,t}=D_jI(t \ge T_0)$. The researcher observes the following:

[^spillover]: This is a common assumption in the synthetic control literature. Recently, @grossi_synthetic_2020 have introduced spillover effects in analyzing new light rail transit.


\begin{align}
Y_{j,t}= \begin{cases}
Y_{j,t}(1) & D_{j,t}=1\\
Y_{j,t}(0) & D_{j,t}=0\\
\end{cases}
\end{align}

The average treatment effect of the treated unit at each period $t$ is denoted $\tau_{t}=\mathbb{E}[Y_{j,t}(1)|D_j=1]-\mathbb{E}[Y_{j,t}(0)|D_j=1]$. In order to draw causal inference, I assume conditional independence on past outcomes:


\begin{assumption}
Conditional Independence on Past Observed Outcomes
\end{assumption}


\begin{align}
\left\{Y_{j,T_0+i}(0),Y_{j,T_0+i}(1)\right\} \indep D_{j,T_0+i} | Y_{j,1},\dots,Y_{j,T_0}
\label{eq:ass_2}
\end{align}



for $i \in \{1,\dots, T-T_0\}$. 


The conditional independence assumption uses the full set of pretreatment outcomes to proxy for the unobserved confounders.

Since we are only interested in the treatment effect post intervention, replace t with $T_0+i$. The estimand can be rewritten as:

$$
\begin{aligned}
\tau_{T_0+i}&=\mathbb{E}[Y_{j,T_0+i}(1)|D_j=1]-\mathbb{E}[Y_{j,T_0+i}(0)|D_j=1]\\
&= \mathbb{E}[Y_{j,T_0+i}(1)|D_j=1]-\mathbb{E}[\mathbb{E}[Y_{j,T_0+i}(0)| Y_{j,1},\dots,Y_{j,T_0}, D_j=1 ]|D_j=1]\\
&= \mathbb{E}[Y_{j,T_0+i}(1)|D_j=1]-\mathbb{E}[\mathbb{E}[Y_{j,T_0+i}(0)| Y_{j,1},\dots,Y_{j,T_0}, D_j=0 ]|D_j=1]\\
&=\mathbb{E}[Y_{j,T_0+i}(1)|D_j=1]-\mathbb{E}[g_{T_0+i}\left(Y_{j,1},\dots,Y_{j,T_0}\right)|D_j=1]
\end{aligned}
$$

where $g_{T_0+i}\left(Y_{j,1},\dots,Y_{j,T_0}\right)=\mathbb{E}[Y_{j,T_0+i}(0)| Y_{j,1},\dots,Y_{j,T_0}, D_j=0 ]$. 

Suppose only one unit, $j=0$, is treated beginning in period $T_0$ and remains treated for all $T_0+i \ge T_0$. Also suppose the other J units are unaffected by the treatment and there is no anticipation effects (SUTVA holds, @rubin_formal_1990)[^spillover]. We observe a sample $\left\{\left(y_{j,1},\dots,y_{j,T},d_j\right)\right\}_{j=0}^J$ from the distribution $\left(Y_{j,1},\dots,Y_{j,T},D_j\right)$. Since there is only one treated unit, $\mathbb{E}[Y_{j,T_0+i}(1)|D_j=1]$ can be estimated with the realization of the data, $y_{0,T_0+i}$, and $\mathbb{E}[g_{T_0+i}\left(Y_{j,1},\dots,Y_{j,T_0}\right)|D_j=1]$ can be estimated as $g_{T_0+i}\left(y_{0,1},\dots,y_{0,T_0}\right)$.

The goal is to estimate the sample ATT:


\begin{align}
\hat{\tau}_{T_0+i}=y_{0,T_0+i}-\hat{g}_{T_0+i}\left(y_{0,1},\dots,y_{0,T_0}\right)
\end{align}

An additional metric of interest in this paper is the sample average treatment effect in the post period:

$$\Delta{\tau}=\frac{1}{T-T_0}\sum_{i=0}^{T-T_0}\tau_{T_0+i}$$

The sample average treatment effect in the post period is estimated as:

$$\hat{\Delta}{\tau}=\frac{1}{T-T_0}\sum_{i=0}^{T-T_0}\hat{\tau}_{T_0+i}$$

## General Model

There are three sources of information to estimate $y_{0,T_0+i}(0)={g}_{T_0+i}\left(y_{0,1},\dots,y_{0,T_0}\right)$: 

i) Untreated pre-treatment units: **y**.

ii) Treated pre-treatment units: $\mathbf{y_0}=\left(y_{0,1},\dots , y_{0,T_0}\right)$.

iii) Untreated post-treatment units.

Creating the counterfactual relies on utilizing all three sources of information. The parameters are estimated in the pre-period and then used to forecast out in the post-period. I propose using a time-varying coefficient model. Namely:

\begin{align}
g_{T_0+i}\left(y_{0,1},\dots,y_{0,T_0}\right)=\sum_{j=1}^{J+1}\beta_{j,T_0+i}y_{j,T_0+i}
\label{eq:mod1}
\end{align}

where $y_{J+1,T_0+i}=1$ for all t (intercept). With a time varying structure, a perfect fit can be made for each $y_{0,t}$. More so, there exists an infinite combination of perfect matches. The model would have no out of sample predictive ability. The perfect fit would have no differentiation between signal and noise. To account for this, I add structure to the time varying coefficients through state space modeling. The coefficients are modeled as random walks. Incorporating \eqref{eq:mod1} into a state space framework and adding the random walk component yields:


\begin{align}
g_{T_0+i}\left(y_{0,1},\dots,y_{0,T_0}\right)&= \sum_{j=1}^{J+1} \beta_{j,T_0+i}y_{j,T_0+i}+\epsilon_{T_0+i} & \epsilon_{T-0+i} \sim \mathcal{N}(0, \sigma^2) &\label{eq:rw1}\\
\beta_{j,T_0+i}&=\beta_{j,T_0+i-1}+\eta_{j,T_0+i} & \eta_{j,T_0+i} \sim \mathcal{N}(0,\theta_j) &\ \ \ \  \forall j \label{eq:rw}\\
\beta_{j,0}&\sim \mathcal{N}(\beta_j,\theta_j P_{jj}) &\ \ \ \ \forall j \label{eq:rw2}
\end{align}

It is common in state space literature to model time varying parameters as random walks [@dangl_predictive_2012; @belmonte_hierarchical_2014; @bitto_achieving_2019]. The choice of random walk allows for a useful decomposition of the coefficients into time-varying and constant components.

The parameters of the model are $\mathbf{v}=\{\sigma^2, \beta_1,...\beta_{J+1},\theta_1,...\theta_{J+1}\}$ with $\epsilon_t$ and $\eta_{j,t}$ assumed independent of all other unknowns. $P_{jj}$ is a hyperparameter set to ensure the initial distribution is disperse. In practice, $P_{jj}$ is set to a very large value creating a diffuse initial state [@durbin_time_2012].

