---
title: "mcmc"
output: pdf_document
---

# The Posterior Estimation (MCMC)

In order to draw the counterfactual, the posterior distribution must be calculated: $P(\tilde{\beta},\beta,\alpha^2, \lambda^2, \sqrt{\theta},\xi^2,\kappa^2, \zeta_{\beta}, \zeta_{\sqrt{\theta}},\sigma^2|\mathbf{y_0}, \mathbf{y})$. With values drawn from the posterior distribution, $\hat{g}_{T_0+i}\left(y_{0,1}(0),\dots,y_{0,T_0}(0)\right)$ can then be estimated for $T_0 + i \ge T_0$. A closed form does not exist for the posterior. Therefore, I implement the Gibbs sampler. After a sufficiently large initial sample, or burn in, the draws from the conditional posterior will be simulations of the joint posterior. 

The posterior estimation can be broken into three main steps:

(i) Estimation of $\tilde{\beta}| \beta,\alpha^2, \lambda^2, \sqrt{\theta},\xi^2,\kappa^2, \zeta_{\beta}, \zeta_{\sqrt{\theta}},\sigma^2, \mathbf{y_0}, \mathbf{y}$.

(ii) Estimation of the parameters: $P(\beta,\alpha^2, \lambda^2, \sqrt{\theta},\xi^2,\kappa^2, \zeta_{\beta}, \zeta_{\sqrt{\theta}},\sigma^2| \mathbf{Y_0},\mathbf{Y})$.

(iii) Estimation of $\hat{g}_{T_0+i}\left(y_{j,1},\dots,y_{j,T_0}\right)$ for $t \ge T_0$.

## Estimation of $\tilde{\beta}| \beta,\alpha^2, \lambda, \sqrt{\theta},\xi^2,\kappa^2, \zeta_{\beta}, \zeta_{\sqrt{\theta}},\sigma^2, \mathbf{y_0}, \mathbf{y}$

Draw $\tilde{\beta}$ using @durbin_simple_2002 for the state space model. First, rewrite equations \eqref{eq:rw1}, \eqref{eq:rw}, and \eqref{eq:rw2} as:

\begin{align}
y'_{0,t}(0) &= \sum_{j=1}^{J+1}\tilde{\beta}_{j,t}z_{j,t} + \epsilon_t & \epsilon_t|\sigma^2 \sim N(0, \sigma^2) \label{eq:mcmckf1}\\
\tilde{\beta}_{j,t}&= \tilde{\beta}_{j,t-1}+\tilde{\eta}_{j,t} & \tilde{\eta}_{j,t} \sim N(0,1) \label{eq:mcmckf2}\\
\tilde{\beta}_{j,0} &\sim N(0,P_{jj}) \label{eq:mcmckf3}
\end{align}


where $z_{j,t}=\sqrt{\theta_j}y_{j,t}$, $y'_{0,t}=y_{0,t}-\sum_{j=1}^{J+1} \beta_j y_{j,t}$.

Many algorithms have been proposed to simulate latent variables in a state space framework. I use the method proposed by @durbin_simple_2002. First run the Kalman filter and smoother given the data and parameters to produce ${\tilde{\beta}_t}^*$. Then simulate new $\tilde{\beta}_{j,t}^+$ and ${y'}_{0,t}^+$ for all j using equations \eqref{eq:mcmckf1}, \eqref{eq:mcmckf2}, and \eqref{eq:mcmckf3}. Then run the Kalman filter and smoother on ${y'}_{0,t}^+(0)$ and $\tilde{\beta}_{j,t}^+$ for all j producing ${\tilde{\beta}_t}^{*,+}$. The new simulated draw of $\tilde{\beta}_t$ (denoted $\tilde{\beta}_t'$) is ${\tilde{\beta}_t'}={\tilde{\beta}_t}^*-\tilde{\beta}_{t}^+ +{\tilde{\beta}_t^{*,+}}$.

## Estimation of the Parameters: $P(\beta,\alpha^2, \lambda, \sqrt{\theta},\xi^2,\kappa^2, \zeta_{\beta}, \zeta_{\sqrt{\theta}},\sigma^2| y_0)$

Attempting to sample $P(\beta,\alpha^2, \lambda^2, \sqrt{\theta},\xi^2,\kappa^2, \zeta_{\beta}, \zeta_{\sqrt{\theta}}, \sigma^2| \mathbf{y_0},\mathbf{y})$ would lead to the same problem as before: no analytic posterior exists. Rather than sampling all parameters at once, I will sample the parameters as blocks. The sampling distributions are derived in the appendix.

### Sample $\beta$ and $\sqrt{\theta}$

Block draw $\beta$ and $\sqrt{\theta}$ from the normal conditional posterior:

\begin{align}
 \mathcal{N}_{2(J+1)}  \left((\tilde{\mathbf{y}}^T\tilde{\mathbf{y}} + \sigma^2 V^{-1})^{-1}\tilde{\mathbf{y}}^T\mathbf{{y_0}},     \sigma^2 (\tilde{\mathbf{y}}^T\tilde{\mathbf{y}} + \sigma^2 V^{-1})^{-1}  \right)
\end{align}



Where:

\begin{align}
\mathbf{\tilde{y}}=\left(\begin{array}{cccccccc} 
y_{1,1} & y_{2,1} & \dots & y_{J+1,1} & \tilde{\beta}_{1,1}y_{1,1} & \tilde{\beta}_{2,1}y_{2,1} & \dots & \tilde{\beta}_{J+1,1}y_{J+1,1}\\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
y_{1,T_0-1} & y_{2,T_0-1} & \dots & y_{J+1,T_0-1} & \tilde{\beta}_{1,T_0-1}y_{1,T_0-1} & \tilde{\beta}_{2,T_0-1}y_{2,T_0-1} & \dots & \tilde{\beta}_{J+1,T_0-1}y_{J+1,T_0-1}
\end{array}\right)
\end{align}



\begin{align}
V= diag\left[\lambda^2 \alpha_{1}^2,\lambda^2\alpha_{2}^2,...,\lambda^2\alpha_{J+1}^2, \kappa^2\xi_1^2,\kappa^2\xi_2^2,\dots,\kappa^2\xi_{J+1}^2\right]
\end{align}

Sampling from sparse matrices can lead preset matrix inversion techniques to fail. To avoid such failures, I implement the algorithm proposed by @bhattacharya_fast_2016.


### Sample $\alpha^2$

Draw $\alpha^2$ using the fact $\frac{1}{\alpha_j^2}$ each have independent inverse-Gaussian (IG) conditional priors:

\begin{align}
IG\left(\sqrt{\frac{2\lambda^2}{\beta_j^2}},2\right) \text{for j=1,...J+1}
\end{align}

### Sample $\lambda^2$

Draw $\lambda^2$ from the conditional inverse gamma prior:

\begin{align}
InverseGamma\left(shape=\frac{J+1}{2}, rate=\frac{1}{\zeta_{\beta}} + \frac{1}{2}\sum_{j=1}^{J+1}\frac{\beta_j^2}{\alpha_j^2} \right)
\end{align}

### Sample $\zeta_{\beta}$

Draw $\zeta_{\beta}$ from the conditional inverse gamma prior:

\begin{align}
InverseGamma\left(1, 1+ \frac{1}{\lambda^2} \right)
\end{align}

### Sample $\xi^2$

Draw $\xi^2$ using the fact $\frac{1}{\xi_j^2}$ each have independent inverse-Gaussian (IG) conditional priors:

\begin{align}
IG\left(\sqrt{\frac{2\kappa^2}{\theta_j}},2\right) \text{for j=1,...J+1}
\end{align}

### Sample $\kappa^2$

Draw $\kappa^2$ from the conditional gamma prior:

\begin{align}
InverseGamma\left(shape=\frac{J+1}{2}, rate= \frac{1}{\zeta_{\sqrt{\theta}}} + \frac{1}{2}\sum_{j=1}^{J+1}\frac{\sqrt{\theta_j}^2}{\xi_j^2} \right)
\end{align}

### Sample $\zeta_{\sqrt{\theta}}$

Draw $\zeta_{\sqrt{\theta}}$ from the conditional inverse gamma prior:

\begin{align}
InverseGamma\left(1, 1+ \frac{1}{\kappa^2} \right)
\end{align}

### Sample $\sigma^2$ {#sigma2}

Draw $\sigma^2$ from the posterior distribution:

\begin{align}
InverseGamma\left(a_1+\frac{T_0-1}{2},a_2+\frac{\sum_{t=1}^{T_0-1}\left(y_{0,t}- \sum_{j=1}^{J+1} \left(\beta_{j}+\tilde{\beta}_{j,t}\sqrt{\theta_j}\right)y_{j,t} \right)^2}{2} \right)
\end{align}

@fruhwirth-schnatter_stochastic_2010 note an identification problem arises when using the non-centered parameterization. There is no way to distinguish between  $\sqrt{\theta_j}\tilde{\beta}_{j,t}$ and $(-\sqrt{\theta_j})(-\tilde{\beta}_{j,t})$. This problem is referred to as *label switching problem*. This issue is a common occurrence in Bayesian estimation when a distribution is multi-modal, as is the case with the square root of a variance. To solve this identification problem, @fruhwirth-schnatter_stochastic_2010 suggest a random sign change at the end of each iteration of the Gibbs Sampler. With 50% chance, the signs on $\tilde{\beta}$ and $\sqrt{\theta}$ are switched. Both @belmonte_hierarchical_2014 and @bitto_achieving_2019 employ this method. 

A final note of interest is the formulation of $\lambda^2$ (and $\kappa^2$). The conditional distribution of $\lambda^2$ relies on $\sum_{j=1}^{J+1} \alpha_j^2$ where each posterior $\alpha_j^2$ relies on $\beta_j$. This direct reliance on $\beta_j$ in the conditional distributions can lead to scaling issues. Data bigger in magnitude can dominate the distribution of $\lambda^2$. The issue of scaling is common in both parametric and nonparametrics shrinkage estimation. To account for this, **all covariates except the intercept are scaled to mean zero variance one** prior to analysis. 




## Sample of $\hat{g}_{T_0+i}\left(y_{0,1},\dots,y_{0,T_0}\right)$ for $t \ge T_0$.

After a sufficiently large burn in period, use the proceeding draws to calculate $\hat{g}_{T_0+i}\left(y_{0,1},\dots,y_{0,T_0}\right)$ for $T_0+i \ge T_0$. Namely, perform the following steps:

(1) Simulate $\tilde{\beta}_{j,t}= \tilde{\beta}_{j,t-1}+\tilde{\eta}_{j,t}$ for all j. Use ${\tilde{\beta}}'_{j,T_0-1}$ simulated in section 4.1 as an initial value. Each iteration of the Gibbs sampler will create a new ${\tilde{\beta}'}_{j,T_0-1}$.

(2) Using the simulated $\tilde{\beta}_{j,t}$, predict $\hat{g}_{T_0+i}\left(y_{0,1}(0),\dots,y_{0,T_0}(0)\right)$ as:

$$\hat{g}_{T_0+i}\left(y_{0,1},\dots,y_{0,T_0}\right)=\sum_{j=1}^{J+1} \left(\beta_{j}+\tilde{\beta}_{j,t}\sqrt{\theta_j}\right)y_{j,t}+\epsilon_t$$
drawing $\epsilon_t \sim N(0, \sigma^2)$. Each iteration of the Gibbs sampler will produce new parameter and state values.

## Estimate $\hat{\tau}_{T_0+i}$ and $\hat{\Delta}_\tau$

After sampling $\hat{g}_{T_0+i}\left(y_{0,1},\dots,y_{0,T_0}\right)$, the sample average treatment effect on the treated is:

\begin{align}
\hat{\tau}_{T_0+i}=y_{0,T_0+i}-\hat{g}_{T_0+i}\left(y_{0,1},\dots,y_{0,T_0}\right)
\end{align}

for $i = \{0,\dots,T-T_0\}$. The sample average treatment effect in the post period is then calculated as:


\begin{align}
\hat{\Delta}_\tau=\frac{1}{T-T_0}\sum_{i=0}^{T-T_0} \hat{\tau}_{T_0+i}
\end{align}

Sampling from the Gibbs Sampler creates an empirical distribution for the treatment effects. Statistical testing can then be performed with the distribution. A major benefit of this approach is valid probability intervals.
