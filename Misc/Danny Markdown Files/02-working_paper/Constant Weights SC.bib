
@article{powell_imperfect_2018,
	title = {Imperfect {Synthetic} {Controls}:},
	abstract = {In 2006, Massachusetts enacted comprehensive health care reform which served as a model for the Aﬀordable Care Act. I study the mortality eﬀects of the reform using synthetic control estimation, relaxing two critical assumptions required to implement this method. The traditional approach assumes the existence of a perfect synthetic control, which cannot exist if the outcomes of the treated unit are outside of the “convex hull” or functions of transitory shocks. I propose simple modiﬁcations to relax these restrictions. The new estimator outperforms the traditional method in simulations. I estimate that the Massachusetts Health Care Reform reduced mortality by 3\%.},
	language = {en},
	author = {Powell, David},
	year = {2018},
	pages = {55},
	file = {Powell - Imperfect Synthetic Controls.pdf:/Users/dannyklinenberg/Zotero/storage/LKY263YQ/Powell - Imperfect Synthetic Controls.pdf:application/pdf}
}

@article{kaul_synthetic_2018,
	title = {Synthetic {Control} {Methods}: {Never} {Use} {All} {Pre}-{Intervention} {Outcomes} {Together} {With} {Covariates}},
	abstract = {It is becoming increasingly popular in applications of synthetic control methods to include the entire pre-treatment path of the outcome variable as economic predictors. We demonstrate both theoretically and empirically that using all outcome lags as separate predictors renders all other covariates irrelevant. This ﬁnding holds irrespective of how important these covariates are for accurately predicting post-treatment values of the outcome, threatening the estimator’s unbiasedness. We show that estimation results and corresponding policy conclusions can change considerably when the usage of outcome lags as predictors is restricted, resulting in other covariates obtaining positive weights. Monte Carlo studies examine potential bias.},
	language = {en},
	author = {Kaul, Ashok and Kloßner, Stefan and Pfeifer, Gregor and Schieler, Manuel},
	year = {2018},
	pages = {24},
	file = {Kaul et al. - Synthetic Control Methods Never Use All Pre-Inter.pdf:/Users/dannyklinenberg/Zotero/storage/NETZYQK2/Kaul et al. - Synthetic Control Methods Never Use All Pre-Inter.pdf:application/pdf}
}

@article{noauthor_shrinkage_nodate,
	title = {Shrinkage {Estimation} of the {Varying} {Coefficient} {Model}},
	language = {en},
	pages = {12},
	file = {Shrinkage Estimation of the Varying Coefficient Mo.pdf:/Users/dannyklinenberg/Zotero/storage/Z8I8GAR6/Shrinkage Estimation of the Varying Coefficient Mo.pdf:application/pdf}
}

@article{kapetanios_time-varying_2018,
	title = {Time-varying {Lasso}},
	volume = {169},
	issn = {01651765},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0165176518301666},
	doi = {10.1016/j.econlet.2018.04.029},
	abstract = {This paper introduces a Lasso-type estimator for large linear models with time-varying parameters. The estimator is easy to implement in practice and standard algorithms developed for Lasso with fixed parameters can be readily used. We derive theoretical properties of the estimator, allowing for deterministic or stochastic smoothly varying parameter processes and discuss ways in which tuning parameters can be data dependent. Monte Carlo simulation and an application to forecasting inflation with macroeconomic variables illustrates the usefulness of our method.},
	language = {en},
	urldate = {2020-04-15},
	journal = {Economics Letters},
	author = {Kapetanios, George and Zikes, Filip},
	month = aug,
	year = {2018},
	pages = {1--6},
	file = {Kapetanios and Zikes - 2018 - Time-varying Lasso.pdf:/Users/dannyklinenberg/Zotero/storage/J6KS87EQ/Kapetanios and Zikes - 2018 - Time-varying Lasso.pdf:application/pdf}
}

@article{athey_matrix_2018,
	title = {Matrix {Completion} {Methods} for {Causal} {Panel} {Data} {Models}},
	url = {http://arxiv.org/abs/1710.10251},
	abstract = {In this paper we study methods for estimating causal eﬀects in settings with panel data, where a subset of units are exposed to a treatment during a subset of periods, and the goal is estimating counterfactual (untreated) outcomes for the treated unit/period combinations. We develop a class of matrix completion estimators that uses the observed elements of the matrix of control outcomes corresponding to untreated unit/periods to predict the “missing” elements of the matrix, corresponding to treated units/periods. The approach estimates a matrix that well-approximates the original (incomplete) matrix, but has lower complexity according to the nuclear norm for matrices. From a technical perspective, we generalize results from the matrix completion literature by allowing the patterns of missing data to have a time series dependency structure. We also present novel insights concerning the connections between the matrix completion literature, the literature on interactive ﬁxed eﬀects models and the literatures on program evaluation under unconfoundedness and synthetic control methods.},
	language = {en},
	urldate = {2020-04-15},
	journal = {arXiv:1710.10251 [econ, math, stat]},
	author = {Athey, Susan and Bayati, Mohsen and Doudchenko, Nikolay and Imbens, Guido and Khosravi, Khashayar},
	month = sep,
	year = {2018},
	note = {arXiv: 1710.10251},
	keywords = {Economics - Econometrics, Mathematics - Statistics Theory},
	file = {Athey et al. - 2018 - Matrix Completion Methods for Causal Panel Data Mo.pdf:/Users/dannyklinenberg/Zotero/storage/GQQYA8J2/Athey et al. - 2018 - Matrix Completion Methods for Causal Panel Data Mo.pdf:application/pdf}
}

@article{xu_generalized_2017,
	title = {Generalized {Synthetic} {Control} {Method}: {Causal} {Inference} with {Interactive} {Fixed} {Effects} {Models}},
	volume = {25},
	issn = {1047-1987, 1476-4989},
	shorttitle = {Generalized {Synthetic} {Control} {Method}},
	url = {https://www.cambridge.org/core/product/identifier/S1047198716000024/type/journal_article},
	doi = {10.1017/pan.2016.2},
	abstract = {Difference-in-differences (DID) is commonly used for causal inference in time-series cross-sectional data. It requires the assumption that the average outcomes of treated and control units would have followed parallel paths in the absence of treatment. In this paper, we propose a method that not only relaxes this often-violated assumption, but also unifies the synthetic control method (Abadie, Diamond, and Hainmueller 2010) with linear fixed effects models under a simple framework, of which DID is a special case. It imputes counterfactuals for each treated unit using control group information based on a linear interactive fixed effects model that incorporates unit-specific intercepts interacted with time-varying coefficients. This method has several advantages. First, it allows the treatment to be correlated with unobserved unit and time heterogeneities under reasonable modeling assumptions. Second, it generalizes the synthetic control method to the case of multiple treated units and variable treatment periods, and improves efficiency and interpretability. Third, with a built-in cross-validation procedure, it avoids specification searches and thus is easy to implement. An empirical example of Election Day Registration and voter turnout in the United States is provided.},
	language = {en},
	number = {1},
	urldate = {2020-04-14},
	journal = {Political Analysis},
	author = {Xu, Yiqing},
	month = jan,
	year = {2017},
	pages = {57--76},
	file = {Xu - 2017 - Generalized Synthetic Control Method Causal Infer.pdf:/Users/dannyklinenberg/Zotero/storage/Y45JN373/Xu - 2017 - Generalized Synthetic Control Method Causal Infer.pdf:application/pdf}
}

@article{fruhwirth-schnatter_stochastic_2010,
	title = {Stochastic model specification search for {Gaussian} and partial non-{Gaussian} state space models},
	volume = {154},
	issn = {03044076},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304407609001614},
	doi = {10.1016/j.jeconom.2009.07.003},
	abstract = {Model specification for state space models is a difficult task as one has to decide which components to include in the model and to specify whether these components are fixed or time-varying. To this aim a new model space MCMC method is developed in this paper. It is based on extending the Bayesian variable selection approach which is usually applied to variable selection in regression models to state space models. For non-Gaussian state space models stochastic model search MCMC makes use of auxiliary mixture sampling. We focus on structural time series models including seasonal components, trend or intervention. The method is applied to various well-known time series.},
	language = {en},
	number = {1},
	urldate = {2020-03-31},
	journal = {Journal of Econometrics},
	author = {Frühwirth-Schnatter, Sylvia and Wagner, Helga},
	month = jan,
	year = {2010},
	pages = {85--100},
	file = {Frühwirth-Schnatter and Wagner - 2010 - Stochastic model specification search for Gaussian.pdf:/Users/dannyklinenberg/Zotero/storage/LB58FX8X/Frühwirth-Schnatter and Wagner - 2010 - Stochastic model specification search for Gaussian.pdf:application/pdf}
}

@article{park_bayesian_2008,
	title = {The {Bayesian} {Lasso}},
	volume = {103},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1198/016214508000000337},
	doi = {10.1198/016214508000000337},
	abstract = {The Lasso estimate for linear regression parameters can be interpreted as a Bayesian posterior mode estimate when the priors on the regression parameters are independent double-exponential (Laplace) distributions. This posterior can also be accessed through a Gibbs sampler using conjugate normal priors for the regression parameters, with independent exponential hyperpriors on their variances. This leads to tractable full conditional distributions through a connection with the inverse Gaussian distribution. Although the Bayesian Lasso does not automatically perform variable selection, it does provide standard errors and Bayesian credible intervals that can guide variable selection. Moreover, the structure of the hierarchical model provides both Bayesian and likelihood methods for selecting the Lasso parameter. The methods described here can also be extended to other Lasso-related estimation methods like bridge regression and robust variants.},
	language = {en},
	number = {482},
	urldate = {2020-03-31},
	journal = {Journal of the American Statistical Association},
	author = {Park, Trevor and Casella, George},
	month = jun,
	year = {2008},
	pages = {681--686},
	file = {Park and Casella - 2008 - The Bayesian Lasso.pdf:/Users/dannyklinenberg/Zotero/storage/SSEBSHZD/Park and Casella - 2008 - The Bayesian Lasso.pdf:application/pdf}
}

@article{belmonte_hierarchical_2014,
	title = {Hierarchical {Shrinkage} in {Time}-{Varying} {Parameter} {Models}: {Hierarchical} {Shrinkage} in {Time}-{Varying} {Parameter} {Models}},
	volume = {33},
	issn = {02776693},
	shorttitle = {Hierarchical {Shrinkage} in {Time}-{Varying} {Parameter} {Models}},
	url = {http://doi.wiley.com/10.1002/for.2276},
	doi = {10.1002/for.2276},
	abstract = {In this paper, we forecast EU area inﬂation with many predictors using time-varying parameter models. The facts that time-varying parameter models are parameter rich and the time span of our data is relatively short motivate a desire for shrinkage. In constant coefﬁcient regression models, the Bayesian Lasso is gaining increasing popularity as an effective tool for achieving such shrinkage. In this paper, we develop econometric methods for using the Bayesian Lasso with time-varying parameter models. Our approach allows for the coefﬁcient on each predictor to be: (i) time varying; (ii) constant over time; or (iii) shrunk to zero. The econometric methodology decides automatically to which category each coefﬁcient belongs. Our empirical results indicate the beneﬁts of such an approach. Copyright © 2013 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {1},
	urldate = {2020-03-31},
	journal = {Journal of Forecasting},
	author = {Belmonte, Miguel A.G. and Koop, Gary and Korobilis, Dimitris},
	month = jan,
	year = {2014},
	pages = {80--94}
}

@article{bitto_achieving_2019,
	title = {Achieving shrinkage in a time-varying parameter model framework},
	volume = {210},
	issn = {03044076},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304407618302070},
	doi = {10.1016/j.jeconom.2018.11.006},
	abstract = {Shrinkage for time-varying parameter (TVP) models is investigated within a Bayesian framework, with the aim to automatically reduce time-varying parameters to static ones, if the model is overfitting. This is achieved through placing the double gamma shrinkage prior on the process variances. An efficient Markov chain Monte Carlo scheme is developed, exploiting boosting based on the ancillarity-sufficiency interweaving strategy. The method is applicable both to TVP models for univariate as well as multivariate time series. Applications include a TVP generalized Phillips curve for EU area inflation modeling and a multivariate TVP Cholesky stochastic volatility model for joint modeling of the returns from the DAX-30 index.},
	language = {en},
	number = {1},
	urldate = {2020-03-31},
	journal = {Journal of Econometrics},
	author = {Bitto, Angela and Frühwirth-Schnatter, Sylvia},
	month = may,
	year = {2019},
	pages = {75--97},
	file = {Bitto and Frühwirth-Schnatter - 2019 - Achieving shrinkage in a time-varying parameter mo.pdf:/Users/dannyklinenberg/Zotero/storage/3JB5963P/Bitto and Frühwirth-Schnatter - 2019 - Achieving shrinkage in a time-varying parameter mo.pdf:application/pdf;Tibsrhani 1994.pdf:/Users/dannyklinenberg/Zotero/storage/ZI6KG6YN/Tibsrhani 1994.pdf:application/pdf}
}

@article{samartsidis_assessing_2019,
	title = {Assessing the {Causal} {Effect} of {Binary} {Interventions} from {Observational} {Panel} {Data} with {Few} {Treated} {Units}},
	volume = {34},
	issn = {0883-4237},
	url = {https://projecteuclid.org/euclid.ss/1570780981},
	doi = {10.1214/19-STS713},
	abstract = {Researchers are often challenged with assessing the impact of an intervention on an outcome of interest in situations where the intervention is nonrandomised, the intervention is only applied to one or few units, the intervention is binary, and outcome measurements are available at multiple time points. In this paper, we review existing methods for causal inference in these situations. We detail the assumptions underlying each method, emphasize connections between the different approaches and provide guidelines regarding their practical implementation. Several open problems are identiﬁed thus highlighting the need for future research.},
	language = {en},
	number = {3},
	urldate = {2020-03-25},
	journal = {Statistical Science},
	author = {Samartsidis, Pantelis and Seaman, Shaun R. and Presanis, Anne M. and Hickman, Matthew and De Angelis, Daniela},
	month = aug,
	year = {2019},
	pages = {486--503},
	file = {Wang 2009.pdf:/Users/dannyklinenberg/Zotero/storage/QTXQSMR7/Wang 2009.pdf:application/pdf;Samartsidis et al. - 2019 - Assessing the Causal Effect of Binary Intervention.pdf:/Users/dannyklinenberg/Zotero/storage/E5AHX8YQ/Samartsidis et al. - 2019 - Assessing the Causal Effect of Binary Intervention.pdf:application/pdf;Arkhangelsky2020.pdf:/Users/dannyklinenberg/Zotero/storage/F6Z5D96B/Arkhangelsky2020.pdf:application/pdf}
}

@article{pesaran_optimal_2013,
	title = {Optimal forecasts in the presence of structural breaks},
	volume = {177},
	issn = {03044076},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304407613000687},
	doi = {10.1016/j.jeconom.2013.04.002},
	abstract = {This paper considers the problem of forecasting under continuous and discrete structural breaks and proposes weighting observations to obtain optimal forecasts in the MSFE sense. We derive optimal weights for one step ahead forecasts. Under continuous breaks, our approach largely recovers exponential smoothing weights. Under discrete breaks, we provide analytical expressions for optimal weights in models with a single regressor, and asymptotically valid weights for models with more than one regressor. It is shown that in these cases the optimal weight is the same across observations within a given regime and differs only across regimes. In practice, where information on structural breaks is uncertain, a forecasting procedure based on robust optimal weights is proposed. The relative performance of our proposed approach is investigated using Monte Carlo experiments and an empirical application to forecasting real GDP using the yield curve across nine industrial economies.},
	language = {en},
	number = {2},
	urldate = {2020-03-23},
	journal = {Journal of Econometrics},
	author = {Pesaran, M. Hashem and Pick, Andreas and Pranovich, Mikhail},
	month = dec,
	year = {2013},
	pages = {134--152},
	file = {Pesaran et al. - 2013 - Optimal forecasts in the presence of structural br.pdf:/Users/dannyklinenberg/Zotero/storage/HIZCG7A6/Pesaran et al. - 2013 - Optimal forecasts in the presence of structural br.pdf:application/pdf}
}

@article{dangl_predictive_2012,
	title = {Predictive regressions with time-varying coefficients},
	volume = {106},
	issn = {0304405X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304405X12000633},
	doi = {10.1016/j.jfineco.2012.04.003},
	abstract = {We evaluate predictive regressions that explicitly consider the time-variation of coefﬁcients in a comprehensive Bayesian framework. For monthly returns of the S\&P 500 index, we demonstrate statistical as well as economic evidence of out-of-sample predictability: relative to an investor using the historic mean, an investor using our methodology could have earned consistently positive utility gains (between 1.8\% and 5.8\% per year over different time periods). We also ﬁnd that predictive models with constant coefﬁcients are dominated by models with time-varying coefﬁcients. Finally, we show a strong link between out-of-sample predictability and the business cycle.},
	language = {en},
	number = {1},
	urldate = {2020-03-23},
	journal = {Journal of Financial Economics},
	author = {Dangl, Thomas and Halling, Michael},
	month = oct,
	year = {2012},
	pages = {157--181},
	file = {Dangl and Halling - 2012 - Predictive regressions with time-varying coefficie.pdf:/Users/dannyklinenberg/Zotero/storage/7DDNDTA4/Dangl and Halling - 2012 - Predictive regressions with time-varying coefficie.pdf:application/pdf}
}

@article{billmeier_assessing_2013,
	title = {Assessing {Economic} {Liberalization} {Episodes}: {A} {Synthetic} {Control} {Approach}},
	volume = {95},
	issn = {0034-6535, 1530-9142},
	shorttitle = {Assessing {Economic} {Liberalization} {Episodes}},
	url = {http://www.mitpressjournals.org/doi/10.1162/REST_a_00324},
	doi = {10.1162/REST_a_00324},
	abstract = {We use a transparent statistical methodology for data-driven case studies—the synthetic control method—to investigate the impact of economic liberalization on real GDP per capita in a worldwide sample of countries. Economic liberalization is measured by a widely used indicator that captures the scope of the market in the economy. The methodology compares the postliberalization GDP trajectory of treated economies with the trajectory of a combination of similar but untreated economies. We ﬁnd that liberalizing the economy had a positive effect in most regions, but more recent liberalizations, in the 1990s and mainly in Africa, had no signiﬁcant impact.},
	language = {en},
	number = {3},
	urldate = {2020-03-14},
	journal = {Review of Economics and Statistics},
	author = {Billmeier, Andreas and Nannicini, Tommaso},
	month = jul,
	year = {2013},
	pages = {983--1001},
	file = {Park 2010.pdf:/Users/dannyklinenberg/Zotero/storage/B9NQHFKZ/Park 2010.pdf:application/pdf;Billmeier and Nannicini - 2013 - Assessing Economic Liberalization Episodes A Synt.pdf:/Users/dannyklinenberg/Zotero/storage/WV23E26B/Billmeier and Nannicini - 2013 - Assessing Economic Liberalization Episodes A Synt.pdf:application/pdf;cunningham2014.pdf:/Users/dannyklinenberg/Zotero/storage/D9VQDESE/cunningham2014.pdf:application/pdf;cunningham2019.pdf:/Users/dannyklinenberg/Zotero/storage/93EP9RD7/cunningham2019.pdf:application/pdf;Kellog_2019.pdf:/Users/dannyklinenberg/Zotero/storage/F2C3LC5X/Kellog_2019.pdf:application/pdf}
}

@article{campos_institutional_2019,
	title = {Institutional integration and economic growth in {Europe}},
	volume = {103},
	issn = {03043932},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304393218301648},
	doi = {10.1016/j.jmoneco.2018.08.001},
	abstract = {The literature on the growth effects of European integration remains inconclusive. This is due to severe methodological diﬃculties mostly driven by country heterogeneity. This paper addresses these concerns using the synthetic control method. It constructs counterfactuals for countries that joined the European Union (EU) from 1973 to 2004. We ﬁnd that growth effects from EU membership are large and positive, with Greece as the exception. Despite substantial variation across countries and over time, we estimate that without European integration, per capita incomes would have been, on average, approximately 10\% lower in the ﬁrst ten years after joining the EU.},
	language = {en},
	urldate = {2020-03-13},
	journal = {Journal of Monetary Economics},
	author = {Campos, Nauro F. and Coricelli, Fabrizio and Moretti, Luigi},
	month = may,
	year = {2019},
	pages = {88--104},
	file = {belmont 2013.pdf:/Users/dannyklinenberg/Zotero/storage/CUIEKH2T/belmont 2013.pdf:application/pdf;Campos et al. - 2019 - Institutional integration and economic growth in E.pdf:/Users/dannyklinenberg/Zotero/storage/6Z3DNGA3/Campos et al. - 2019 - Institutional integration and economic growth in E.pdf:application/pdf}
}

@article{abadie_synthetic_2010,
	title = {Synthetic {Control} {Methods} for {Comparative} {Case} {Studies}: {Estimating} the {Effect} of {California}’s {Tobacco} {Control} {Program}},
	volume = {105},
	issn = {0162-1459, 1537-274X},
	shorttitle = {Synthetic {Control} {Methods} for {Comparative} {Case} {Studies}},
	url = {http://www.tandfonline.com/doi/abs/10.1198/jasa.2009.ap08746},
	doi = {10.1198/jasa.2009.ap08746},
	language = {en},
	number = {490},
	urldate = {2020-03-13},
	journal = {Journal of the American Statistical Association},
	author = {Abadie, Alberto and Diamond, Alexis and Hainmueller, Jens},
	month = jun,
	year = {2010},
	pages = {493--505},
	file = {Abadie et al. - 2010 - Synthetic Control Methods for Comparative Case Stu.pdf:/Users/dannyklinenberg/Zotero/storage/WJ3D9UYU/Abadie et al. - 2010 - Synthetic Control Methods for Comparative Case Stu.pdf:application/pdf}
}

@article{abadie_economic_2003,
	title = {The {Economic} {Costs} of {Conflict}: {A} {Case} {Study} of the {Basque} {Country}},
	volume = {93},
	issn = {0002-8282},
	shorttitle = {The {Economic} {Costs} of {Conflict}},
	url = {http://pubs.aeaweb.org/doi/10.1257/000282803321455188},
	doi = {10.1257/000282803321455188},
	language = {en},
	number = {1},
	urldate = {2020-03-13},
	journal = {American Economic Review},
	author = {Abadie, Alberto and Gardeazabal, Javier},
	month = feb,
	year = {2003},
	pages = {113--132},
	file = {Abadie and Gardeazabal - 2003 - The Economic Costs of Conflict A Case Study of th.pdf:/Users/dannyklinenberg/Zotero/storage/TFIH3NT5/Abadie and Gardeazabal - 2003 - The Economic Costs of Conflict A Case Study of th.pdf:application/pdf}
}

@article{athey_state_2017,
	title = {The {State} of {Applied} {Econometrics}: {Causality} and {Policy} {Evaluation}},
	volume = {31},
	issn = {0895-3309},
	shorttitle = {The {State} of {Applied} {Econometrics}},
	url = {http://pubs.aeaweb.org/doi/10.1257/jep.31.2.3},
	doi = {10.1257/jep.31.2.3},
	language = {en},
	number = {2},
	urldate = {2020-03-13},
	journal = {Journal of Economic Perspectives},
	author = {Athey, Susan and Imbens, Guido W.},
	month = may,
	year = {2017},
	pages = {3--32},
	file = {Athey and Imbens - 2017 - The State of Applied Econometrics Causality and P.pdf:/Users/dannyklinenberg/Zotero/storage/E398F4MV/Athey and Imbens - 2017 - The State of Applied Econometrics Causality and P.pdf:application/pdf}
}

@book{durbin_time_2012,
	address = {Oxford},
	edition = {2nd ed},
	series = {Oxford statistical science series},
	title = {Time series analysis by state space methods},
	isbn = {978-0-19-964117-8},
	number = {38},
	publisher = {Oxford University Press},
	author = {Durbin, J. and Koopman, S. J.},
	year = {2012},
	keywords = {State-space methods, Time-series analysis},
	file = {Durbin and Koopman - 2012 - Time series analysis by state space methods.pdf:/Users/dannyklinenberg/Zotero/storage/HABW65UP/Durbin and Koopman - 2012 - Time series analysis by state space methods.pdf:application/pdf}
}

@techreport{scott_bayesian_2013,
	address = {Cambridge, MA},
	title = {Bayesian {Variable} {Selection} for {Nowcasting} {Economic} {Time} {Series}},
	url = {http://www.nber.org/papers/w19567.pdf},
	language = {en},
	number = {w19567},
	urldate = {2020-03-13},
	institution = {National Bureau of Economic Research},
	author = {Scott, Steven and Varian, Hal},
	month = oct,
	year = {2013},
	doi = {10.3386/w19567},
	pages = {w19567},
	file = {Scott and Varian - 2013 - Bayesian Variable Selection for Nowcasting Economi.pdf:/Users/dannyklinenberg/Zotero/storage/RXQG28PE/Scott and Varian - 2013 - Bayesian Variable Selection for Nowcasting Economi.pdf:application/pdf}
}

@article{scott_predicting_nodate,
	title = {Predicting the {Present} with {Bayesian} {Structural} {Time} {Series}},
	abstract = {This article describes a system for short term forecasting based on an ensemble prediction that averages over diﬀerent combinations of predictors. The system combines a structural time series model for the target series with regression component capturing the contributions of contemporaneous search query data. A spike-and-slab prior on the regression coeﬃcients induces sparsity, dramatically reducing the size of the regression problem. Our system averages over potential contributions from a very large set of models and gives easily digested reports of which coeﬃcients are likely to be important. We illustrate with applications to initial claims for unemployment beneﬁts and to retail sales. Although our exposition focuses on using search engine data to forecast economic time series, the underlying statistical methods can be applied to more general short term forecasting with large numbers of contemporaneous predictors.},
	language = {en},
	author = {Scott, Steven L and Varian, Hal},
	pages = {21},
	file = {Scott and Varian - Predicting the Present with Bayesian Structural Ti.pdf:/Users/dannyklinenberg/Zotero/storage/6R4353UZ/Scott and Varian - Predicting the Present with Bayesian Structural Ti.pdf:application/pdf}
}

@article{brodersen_inferring_2015,
	title = {Inferring causal impact using {Bayesian} structural time-series models},
	volume = {9},
	issn = {1932-6157},
	url = {http://projecteuclid.org/euclid.aoas/1430226092},
	doi = {10.1214/14-AOAS788},
	language = {en},
	number = {1},
	urldate = {2020-03-13},
	journal = {The Annals of Applied Statistics},
	author = {Brodersen, Kay H. and Gallusser, Fabian and Koehler, Jim and Remy, Nicolas and Scott, Steven L.},
	month = mar,
	year = {2015},
	pages = {247--274},
	file = {belmont 2013.pdf:/Users/dannyklinenberg/Zotero/storage/2VNEXMVC/belmont 2013.pdf:application/pdf;Brodersen et al. - 2015 - Inferring causal impact using Bayesian structural .pdf:/Users/dannyklinenberg/Zotero/storage/EDZG5GHG/Brodersen et al. - 2015 - Inferring causal impact using Bayesian structural .pdf:application/pdf}
}

@article{kinn_synthetic_2018,
	title = {Synthetic {Control} {Methods} and {Big} {Data}},
	url = {http://arxiv.org/abs/1803.00096},
	abstract = {Many macroeconomic policy questions may be assessed in a case study framework, where the time series of a treated unit is compared to a counterfactual constructed from a large pool of control units. I provide a general framework for this setting, tailored to predict the counterfactual by minimizing a tradeoﬀ between underﬁtting (bias) and overﬁtting (variance). The framework nests recently proposed structural and reduced form machine learning approaches as special cases. Furthermore, diﬀerence-in-diﬀerences with matching and the original synthetic control are restrictive cases of the framework, in general not minimizing the bias-variance objective. Using simulation studies I ﬁnd that machine learning methods outperform traditional methods when the number of potential controls is large or the treated unit is substantially diﬀerent from the controls. Equipped with a toolbox of approaches, I revisit a study on the eﬀect of economic liberalisation on economic growth. I ﬁnd eﬀects for several countries where no eﬀect was found in the original study. Furthermore, I inspect how a systematically important bank respond to increasing capital requirements by using a large pool of banks to estimate the counterfactual. Finally, I assess the eﬀect of a changing product price on product sales using a novel scanner dataset.},
	language = {en},
	urldate = {2020-03-13},
	journal = {arXiv:1803.00096 [econ]},
	author = {Kinn, Daniel},
	month = feb,
	year = {2018},
	note = {arXiv: 1803.00096},
	keywords = {Economics - Econometrics},
	file = {Kinn - 2018 - Synthetic Control Methods and Big Data.pdf:/Users/dannyklinenberg/Zotero/storage/38X76Z8P/Kinn - 2018 - Synthetic Control Methods and Big Data.pdf:application/pdf}
}

@article{botosaru_role_2019,
	title = {On the role of covariates in the synthetic control method},
	issn = {1368-4221, 1368-423X},
	url = {https://academic.oup.com/ectj/advance-article/doi/10.1093/ectj/utz001/5303851},
	doi = {10.1093/ectj/utz001},
	abstract = {Abadie et al. (2010) derive bounds on the bias of the synthetic control estimator under a perfect balance assumption on both observed covariates and pre-treatment outcomes. In the absence of a perfect balance on covariates, we show that it is still possible to derive such bounds, albeit at the expense of relying on stronger assumptions about the effects of observed and unobserved covariates and of generating looser bounds. We also show that a perfect balance on pre-treatment outcomes does not generally imply an approximate balance for all covariates, even when they are all relevant. Our results have important implications for the implementation of the method.},
	language = {en},
	urldate = {2020-03-13},
	journal = {The Econometrics Journal},
	author = {Botosaru, Irene and Ferman, Bruno},
	month = jan,
	year = {2019},
	pages = {utz001},
	file = {Botosaru and Ferman - 2019 - On the role of covariates in the synthetic control.pdf:/Users/dannyklinenberg/Zotero/storage/UYSSGSYY/Botosaru and Ferman - 2019 - On the role of covariates in the synthetic control.pdf:application/pdf;Gobillon 2016.pdf:/Users/dannyklinenberg/Zotero/storage/CBBQ8TSC/Gobillon 2016.pdf:application/pdf}
}

@techreport{doudchenko_balancing_2016,
	address = {Cambridge, MA},
	title = {Balancing, {Regression}, {Difference}-{In}-{Differences} and {Synthetic} {Control} {Methods}: {A} {Synthesis}},
	shorttitle = {Balancing, {Regression}, {Difference}-{In}-{Differences} and {Synthetic} {Control} {Methods}},
	url = {http://www.nber.org/papers/w22791.pdf},
	abstract = {In a seminal paper Abadie et al (2010) develop the synthetic control procedure for estimating the effect of a treatment, in the presence of a single treated unit and a number of control units, with pre-treatment outcomes observed for all units. The method constructs a set of weights such that covariates and pre-treatment outcomes of the treated unit are approximately matched by a weighted average of control units. The weights are restricted to be nonnegative and sum to one, which allows the procedure to obtain the weights even when the number of lagged outcomes is modest relative to the number of control units, a setting that is not uncommon in applications. In the current paper we propose a more general class of synthetic control estimators that allows researchers to relax some of the restrictions in the ADH method. We allow the weights to be negative, do not necessarily restrict the sum of the weights, and allow for a permanent additive difference between the treated unit and the controls, similar to difference-in-difference procedures. The weights directly minimize the distance between the lagged outcomes for the treated and the control units, using regularization methods to deal with a potentially large number of possible control units.},
	language = {en},
	number = {w22791},
	urldate = {2020-03-13},
	institution = {National Bureau of Economic Research},
	author = {Doudchenko, Nikolay and Imbens, Guido},
	month = oct,
	year = {2016},
	doi = {10.3386/w22791},
	pages = {w22791},
	file = {Doudchenko and Imbens - 2016 - Balancing, Regression, Difference-In-Differences a.pdf:/Users/dannyklinenberg/Zotero/storage/EVC9U8YA/Doudchenko and Imbens - 2016 - Balancing, Regression, Difference-In-Differences a.pdf:application/pdf}
}

@article{makalic_high-dimensional_2016,
	title = {High-{Dimensional} {Bayesian} {Regularised} {Regression} with the {BayesReg} {Package}},
	url = {http://arxiv.org/abs/1611.06649},
	abstract = {Bayesian penalized regression techniques, such as the Bayesian lasso and the Bayesian horseshoe estimator, have recently received a significant amount of attention in the statistics literature. However, software implementing state-of-the-art Bayesian penalized regression, outside of general purpose Markov chain Monte Carlo platforms such as STAN, is relatively rare. This paper introduces bayesreg, a new toolbox for fitting Bayesian penalized regression models with continuous shrinkage prior densities. The toolbox features Bayesian linear regression with Gaussian or heavy-tailed error models and Bayesian logistic regression with ridge, lasso, horseshoe and horseshoe\$+\$ estimators. The toolbox is free, open-source and available for use with the MATLAB and R numerical platforms.},
	urldate = {2020-05-13},
	journal = {arXiv:1611.06649 [stat]},
	author = {Makalic, Enes and Schmidt, Daniel F.},
	month = dec,
	year = {2016},
	note = {arXiv: 1611.06649},
	keywords = {Statistics - Computation},
	annote = {Comment: 17 pages, 1 figure},
	file = {arXiv Fulltext PDF:/Users/dannyklinenberg/Zotero/storage/WM2ZTMIV/Makalic and Schmidt - 2016 - High-Dimensional Bayesian Regularised Regression w.pdf:application/pdf;arXiv.org Snapshot:/Users/dannyklinenberg/Zotero/storage/6WH7WBGX/1611.html:text/html}
}

@article{bhattacharya_fast_2016,
	title = {Fast sampling with {Gaussian} scale-mixture priors in high-dimensional regression},
	url = {http://arxiv.org/abs/1506.04778},
	abstract = {We propose an efficient way to sample from a class of structured multivariate Gaussian distributions which routinely arise as conditional posteriors of model parameters that are assigned a conditionally Gaussian prior. The proposed algorithm only requires matrix operations in the form of matrix multiplications and linear system solutions. We exhibit that the computational complexity of the proposed algorithm grows linearly with the dimension unlike existing algorithms relying on Cholesky factorizations with cubic orders of complexity. The algorithm should be broadly applicable in settings where Gaussian scale mixture priors are used on high dimensional model parameters. We provide an illustration through posterior sampling in a high dimensional regression setting with a horseshoe prior on the vector of regression coefficients.},
	urldate = {2020-05-14},
	journal = {arXiv:1506.04778 [stat]},
	author = {Bhattacharya, Anirban and Chakraborty, Antik and Mallick, Bani K.},
	month = jun,
	year = {2016},
	note = {arXiv: 1506.04778},
	keywords = {Statistics - Computation},
	file = {arXiv Fulltext PDF:/Users/dannyklinenberg/Zotero/storage/SD39G9RL/Bhattacharya et al. - 2016 - Fast sampling with Gaussian scale-mixture priors i.pdf:application/pdf;arXiv.org Snapshot:/Users/dannyklinenberg/Zotero/storage/C753G5Q9/1506.html:text/html}
}

@article{abadie_using_2019,
	title = {Using {Synthetic} {Controls}: {Feasibility}, {Data} {Requirements}, and {Methodological} {Aspects}},
	language = {en},
	author = {Abadie, Alberto},
	year = {2019},
	pages = {44},
	file = {Abadie - Using Synthetic Controls Feasibility, Data Requir.pdf:/Users/dannyklinenberg/Zotero/storage/KXXVGN4Q/Abadie - Using Synthetic Controls Feasibility, Data Requir.pdf:application/pdf}
}

@article{tibshirani_regression_1996,
	title = {Regression {Shrinkage} and {Selection} via the {Lasso}},
	volume = {58},
	url = {http://www.jstor.org/stable/2346178},
	abstract = {We propose a new methodfor estimationin linear models. The 'lasso' minimizesthe residualsumofsquaressubjectto thesumoftheabsolutevalueofthecoefficientbseingless than a constant.Because of the nature of this constraintit tends to produce some coefficienttshatare exactly0 and hencegivesinterpretablme odels.Our simulationstudies suggestthatthelasso enjoyssomeofthefavourablepropertiesofbothsubsetselectionand ridgeregressionI.t producesinterpretablme odels like subsetselectionand exhibitsthe stabilityof ridgeregressionT. hereis also an interestinrgelationshipwithrecentworkin adaptivefunctionestimationbyDonoho and JohnstoneT. he lasso idea is quitegeneraland can be appliedin a varietyofstatisticaml odels:extensionstogeneralizedregressionmodels and tree-basedmodelsare brieflydescribed.},
	language = {en},
	number = {1},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Tibshirani, Robert},
	year = {1996},
	pages = {267--288},
	file = {Tibsrhani 1994.pdf:/Users/dannyklinenberg/OneDrive/Documents/School Stuff/UCSB/Year 2/Winter/Time Series/SC Proposal/Synthetic Controls/Tibsrhani 1994.pdf:application/pdf}
}

@article{polson_half-cauchy_2011,
	title = {On the half-{Cauchy} prior for a global scale parameter},
	url = {http://arxiv.org/abs/1104.4937},
	abstract = {This paper argues that the half-Cauchy distribution should replace the inverse-Gamma distribution as a default prior for a top-level scale parameter in Bayesian hierarchical models, at least for cases where a proper prior is necessary. Our arguments involve a blend of Bayesian and frequentist reasoning, and are intended to complement the original case made by Gelman (2006) in support of the folded-t family of priors. First, we generalize the half-Cauchy prior to the wider class of hypergeometric inverted-beta priors. We derive expressions for posterior moments and marginal densities when these priors are used for a top-level normal variance in a Bayesian hierarchical model. We go on to prove a proposition that, together with the results for moments and marginals, allows us to characterize the frequentist risk of the Bayes estimators under all global-shrinkage priors in the class. These theoretical results, in turn, allow us to study the frequentist properties of the half-Cauchy prior versus a wide class of alternatives. The half-Cauchy occupies a sensible 'middle ground' within this class: it performs very well near the origin, but does not lead to drastic compromises in other parts of the parameter space. This provides an alternative, classical justification for the repeated, routine use of this prior. We also consider situations where the underlying mean vector is sparse, where we argue that the usual conjugate choice of an inverse-gamma prior is particularly inappropriate, and can lead to highly distorted posterior inferences. Finally, we briefly summarize some open issues in the specification of default priors for scale terms in hierarchical models.},
	urldate = {2020-05-20},
	journal = {arXiv:1104.4937 [stat]},
	author = {Polson, Nicholas G. and Scott, James G.},
	month = sep,
	year = {2011},
	note = {arXiv: 1104.4937},
	keywords = {Statistics - Methodology},
	file = {arXiv Fulltext PDF:/Users/dannyklinenberg/Zotero/storage/YFYBUPG5/Polson and Scott - 2011 - On the half-Cauchy prior for a global scale parame.pdf:application/pdf;arXiv.org Snapshot:/Users/dannyklinenberg/Zotero/storage/ZQS6VJTE/1104.html:text/html}
}

@book{hastie_statistical_2015,
	address = {Boca Raton},
	series = {Monographs on statistics and applied probability},
	title = {Statistical learning with sparsity: the lasso and generalizations},
	isbn = {978-1-4987-1216-3},
	shorttitle = {Statistical learning with sparsity},
	number = {143},
	publisher = {CRC Press, Taylor \& Francis Group},
	author = {Hastie, Trevor and Tibshirani, Robert and Wainwright, Martin},
	year = {2015},
	keywords = {Mathematical statistics, Least squares, Linear models (Statistics), Proof theory},
	annote = {"A Chapman \& Hall book."}
}

@incollection{bernardo_shrink_2011,
	title = {Shrink {Globally}, {Act} {Locally}: {Sparse} {Bayesian} {Regularization} and {Prediction}*},
	isbn = {978-0-19-969458-7},
	shorttitle = {Shrink {Globally}, {Act} {Locally}},
	url = {http://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780199694587.001.0001/acprof-9780199694587-chapter-17},
	abstract = {We study the classic problem of choosing a prior distribution for a location parameter β = (β1, . . . , βp) as p grows large. First, we study the standard “global-local shrinkage” approach, based on scale mixtures of normals. Two theorems are presented which characterize certain desirable properties of shrinkage priors for sparse problems. Next, we review some recent results showing how L´evy processes can be used to generate inﬁnite-dimensional versions of standard normal scale-mixture priors, along with new priors that have yet to be seriously studied in the literature. This approach provides an intuitive framework both for generating new regularization penalties and shrinkage rules, and for performing asymptotic analysis on existing models.},
	language = {en},
	urldate = {2020-05-29},
	booktitle = {Bayesian {Statistics} 9},
	publisher = {Oxford University Press},
	author = {Polson, Nicholas G. and Scott, James G.},
	editor = {Bernardo, José M. and Bayarri, M. J. and Berger, James O. and Dawid, A. P. and Heckerman, David and Smith, Adrian F. M. and West, Mike},
	month = oct,
	year = {2011},
	doi = {10.1093/acprof:oso/9780199694587.003.0017},
	doi = {10.1093/acprof:oso/9780199694587.003.0017},
	pages = {501--538},
	file = {Polson and Scott - 2011 - Shrink Globally, Act Locally Sparse Bayesian Regu.pdf:/Users/dannyklinenberg/Zotero/storage/LSQ42VUH/Polson and Scott - 2011 - Shrink Globally, Act Locally Sparse Bayesian Regu.pdf:application/pdf}
}

@article{huber_inducing_2019,
	title = {Inducing {Sparsity} and {Shrinkage} in {Time}-{Varying} {Parameter} {Models}},
	url = {http://arxiv.org/abs/1905.10787},
	abstract = {Time-varying parameter (TVP) models have the potential to be over-parameterized, particularly when the number of variables in the model is large. Global-local priors are increasingly used to induce shrinkage in such models. But the estimates produced by these priors can still have appreciable uncertainty. Sparsification has the potential to reduce this uncertainty and improve forecasts. In this paper, we develop computationally simple methods which both shrink and sparsify TVP models. In a simulated data exercise we show the benefits of our shrink-then-sparsify approach in a variety of sparse and dense TVP regressions. In a macroeconomic forecasting exercise, we find our approach to substantially improve forecast performance relative to shrinkage alone.},
	urldate = {2020-05-30},
	journal = {arXiv:1905.10787 [econ]},
	author = {Huber, Florian and Koop, Gary and Onorante, Luca},
	month = dec,
	year = {2019},
	note = {arXiv: 1905.10787},
	keywords = {Economics - Econometrics},
	file = {arXiv Fulltext PDF:/Users/dannyklinenberg/Zotero/storage/XBT4TLME/Huber et al. - 2019 - Inducing Sparsity and Shrinkage in Time-Varying Pa.pdf:application/pdf;arXiv.org Snapshot:/Users/dannyklinenberg/Zotero/storage/ZWMCIPVI/1905.html:text/html}
}

@article{durbin_simple_2002,
	title = {A simple and efficient simulation smoother for state space time series analysis},
	volume = {89},
	issn = {0006-3444, 1464-3510},
	url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/89.3.603},
	doi = {10.1093/biomet/89.3.603},
	abstract = {A simulation smoother in state space time series analysis is a procedure for drawing samples from the conditional distribution of state or disturbance vectors given the observations. We present a new technique for this which is both simple and computationally efficient. The treatment includes models with diffuse initial conditions and regression effects. Computational comparisons are made with the previous standard method. Two applications are provided to illustrate the use of the simulation smoother for Gibbs sampling for Bayesian inference and importance sampling for classical inference.},
	language = {en},
	number = {3},
	urldate = {2020-05-31},
	journal = {Biometrika},
	author = {Durbin, J. and Koopman, S. J.},
	month = aug,
	year = {2002},
	pages = {603--616},
	file = {Durbin - 2002 - A simple and efficient simulation smoother for sta.pdf:/Users/dannyklinenberg/Zotero/storage/QLQ6FTV8/Durbin - 2002 - A simple and efficient simulation smoother for sta.pdf:application/pdf}
}

@incollection{gelman_bayesian_2014,
	address = {Boca Raton},
	edition = {Third edition},
	series = {Chapman \& {Hall}/{CRC} texts in statistical science},
	title = {Bayesian {Data} {Analysis}},
	isbn = {978-1-4398-4095-5},
	abstract = {"Preface This book is intended to have three roles and to serve three associated audiences: an introductory text on Bayesian inference starting from first principles, a graduate text on effective current approaches to Bayesian modeling and computation in statistics and related fields, and a handbook of Bayesian methods in applied statistics for general users of and researchers in applied statistics. Although introductory in its early sections, the book is definitely not elementary in the sense of a first text in statistics. The mathematics used in our book is basic probability and statistics, elementary calculus, and linear algebra. A review of probability notation is given in Chapter 1 along with a more detailed list of topics assumed to have been studied. The practical orientation of the book means that the reader's previous experience in probability, statistics, and linear algebra should ideally have included strong computational components. To write an introductory text alone would leave many readers with only a taste of the conceptual elements but no guidance for venturing into genuine practical applications, beyond those where Bayesian methods agree essentially with standard non-Bayesian analyses. On the other hand, we feel it would be a mistake to present the advanced methods without first introducing the basic concepts from our data-analytic perspective. Furthermore, due to the nature of applied statistics, a text on current Bayesian methodology would be incomplete without a variety of worked examples drawn from real applications. To avoid cluttering the main narrative, there are bibliographic notes at the end of each chapter and references at the end of the book"--},
	booktitle = {Bayesian data analysis},
	publisher = {CRC Press},
	author = {Gelman, Andrew},
	year = {2014},
	keywords = {Bayesian statistical decision theory, MATHEMATICS / Probability \& Statistics / General},
	pages = {585--588}
}

@article{samartsidis_bayesian_2020,
	title = {A {Bayesian} multivariate factor analysis model for evaluating an intervention by using observational time series data on multiple outcomes},
	issn = {0964-1998, 1467-985X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/rssa.12569},
	doi = {10.1111/rssa.12569},
	abstract = {A problem that is frequently encountered in many areas of scientiﬁc research is that of estimating the effect of a non-randomized binary intervention on an outcome of interest by using time series data on units that received the intervention (‘treated’) and units that did not (‘controls’). One popular estimation method in this setting is based on the factor analysis (FA) model. The FA model is ﬁtted to the preintervention outcome data on treated units and all the outcome data on control units, and the counterfactual treatment-free post-intervention outcomes of the former are predicted from the ﬁtted model. Intervention effects are estimated as the observed outcomes minus these predicted counterfactual outcomes. We propose a model that extends the FA model for estimating intervention effects by jointly modelling the multiple outcomes to exploit shared variability, and assuming an auto-regressive structure on factors to account for temporal correlations in the outcome. Using simulation studies, we show that the method proposed can improve the precision of the intervention effect estimates and achieve better control of the type I error rate (compared with the FA model), especially when either the number of preintervention measurements or the number of control units is small. We apply our method to estimate the effect of stricter alcohol licensing policies on alcohol-related harms.},
	language = {en},
	urldate = {2020-07-02},
	journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
	author = {Samartsidis, Pantelis and Seaman, Shaun R. and Montagna, Silvia and Charlett, André and Hickman, Matthew and Angelis, Daniela De},
	month = may,
	year = {2020},
	pages = {rssa.12569},
	file = {Samartsidis et al. - 2020 - A Bayesian multivariate factor analysis model for .pdf:/Users/dannyklinenberg/Zotero/storage/9QUAABUZ/Samartsidis et al. - 2020 - A Bayesian multivariate factor analysis model for .pdf:application/pdf}
}

@article{li_statistical_2019,
	title = {Statistical {Inference} for {Average} {Treatment} {Effects} {Estimated} by {Synthetic} {Control} {Methods}},
	issn = {0162-1459, 1537-274X},
	url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2019.1686986},
	doi = {10.1080/01621459.2019.1686986},
	abstract = {The synthetic control (SC) method, a powerful tool for estimating average treatment effects (ATE), is increasingly popular in fields such as statistics, economics, political science, and marketing. The SC is particularly suitable for estimating ATE with a single (or a few) treated unit(s), a fixed number of control units, and large pre and post-treatment periods (which we refer as “long panels”). To date, there has been no formal inference theory for SC ATE estimator with long panels under general conditions. Existing work mostly use placebo tests for inference or some permutation methods when the post-treatment period is small. In this article, we derive the asymptotic distribution of the SC and modified synthetic control (MSC) ATE estimators using projection theory. We show that a properly designed subsampling method can be used to obtain confidence intervals and conduct inference whereas the standard bootstrap cannot. Simulations and an empirical application that examines the effect of opening a physical showroom by an e-tailer demonstrate the usefulness of the MSC method in applications. Supplementary materials for this article are available online.},
	language = {en},
	urldate = {2020-07-02},
	journal = {Journal of the American Statistical Association},
	author = {Li, Kathleen T.},
	month = dec,
	year = {2019},
	pages = {1--16},
	file = {Li - 2019 - Statistical Inference for Average Treatment Effect.pdf:/Users/dannyklinenberg/Zotero/storage/X4YVJQ3W/Li - 2019 - Statistical Inference for Average Treatment Effect.pdf:application/pdf}
}

@article{chernozhukov_exact_2019,
	title = {An {Exact} and {Robust} {Conformal} {Inference} {Method} for {Counterfactual} and {Synthetic} {Controls}},
	url = {http://arxiv.org/abs/1712.09089},
	abstract = {We introduce new inference procedures for counterfactual and synthetic control methods for policy evaluation. Our methods work in conjunction with many different approaches for predicting counterfactual mean outcomes in the absence of a policy intervention. Examples include synthetic controls, difference-in-differences, factor and matrix completion models, and (fused) time series panel data models. The proposed procedures are valid under weak and easy-to-verify conditions and are provably robust against misspecification. Our approach demonstrates an excellent small-sample performance in simulations and is taken to a data application where we re-evaluate the consequences of decriminalizing indoor prostitution.},
	urldate = {2020-07-09},
	journal = {arXiv:1712.09089 [econ, stat]},
	author = {Chernozhukov, Victor and Wuthrich, Kaspar and Zhu, Yinchu},
	month = nov,
	year = {2019},
	note = {arXiv: 1712.09089},
	keywords = {Economics - Econometrics, Statistics - Methodology},
	file = {arXiv Fulltext PDF:/Users/dannyklinenberg/Zotero/storage/7J36XIZ4/Chernozhukov et al. - 2019 - An Exact and Robust Conformal Inference Method for.pdf:application/pdf;arXiv.org Snapshot:/Users/dannyklinenberg/Zotero/storage/ZYDDQBQR/1712.html:text/html}
}

@article{grossi_synthetic_2020,
	title = {Synthetic {Control} {Group} {Methods} in the {Presence} of {Interference}: {The} {Direct} and {Spillover} {Effects} of {Light} {Rail} on {Neighborhood} {Retail} {Activity}},
	shorttitle = {Synthetic {Control} {Group} {Methods} in the {Presence} of {Interference}},
	url = {http://arxiv.org/abs/2004.05027},
	abstract = {In recent years, Synthetic Control Group (SCG) methods have received great attention from scholars and have been subject to extensions and comparisons with alternative approaches for program evaluation. However, the existing methodological literature mainly relies on the assumption of non-interference. We propose to generalize the SCG method to studies where interference between the treated and the untreated units is plausible. We frame our discussion in the potential outcomes approach. Under a partial interference assumption, we formally deﬁne relevant direct and spillover eﬀects. We also consider the “unrealized” spillover eﬀect on the treated unit in the hypothetical scenario that another unit in the treated unit’s neighborhood had been assigned to the intervention. Then we investigate the assumptions under which we can identify and estimate the causal eﬀects of interest, and show how they can be estimated using the SCG method. We apply our approach to the analysis of an observational study, where the focus is on assessing direct and spillover causal eﬀects of a new light rail line recently built in Florence (Italy) on the commercial vitality of the street where it was built and of the streets in the treated street’s neighborhood.},
	language = {en},
	urldate = {2020-07-12},
	journal = {arXiv:2004.05027 [econ, stat]},
	author = {Grossi, Giulio and Lattarulo, Patrizia and Mariani, Marco and Mattei, Alessandra and Öner, Özge},
	month = jun,
	year = {2020},
	note = {arXiv: 2004.05027},
	keywords = {Economics - Econometrics, Statistics - Applications},
	file = {Grossi et al. - 2020 - Synthetic Control Group Methods in the Presence of.pdf:/Users/dannyklinenberg/Zotero/storage/HMPP74SL/Grossi et al. - 2020 - Synthetic Control Group Methods in the Presence of.pdf:application/pdf}
}

@article{hsiao_panel_2012,
	title = {A {PANEL} {DATA} {APPROACH} {FOR} {PROGRAM} {EVALUATION}: {MEASURING} {THE} {BENEFITS} {OF} {POLITICAL} {AND} {ECONOMIC} {INTEGRATION} {OF} {HONG} {KONG} {WITH} {MAINLAND} {CHINA}},
	volume = {27},
	issn = {08837252},
	shorttitle = {A {PANEL} {DATA} {APPROACH} {FOR} {PROGRAM} {EVALUATION}},
	url = {http://doi.wiley.com/10.1002/jae.1230},
	doi = {10.1002/jae.1230},
	abstract = {We propose a simple to implement panel data method to evaluate the impacts of social policy. The basic idea is to exploit the dependence among cross-sectional units to construct the counterfactuals. The cross-sectional correlations are attributed to the presence of some (unobserved) common factors. However, instead of trying to estimate the unobserved factors, we propose to use observed data. We use a panel of 24 countries to evaluate the impact of political and economic integration of Hong Kong (HK) with Mainland China. We ﬁnd that the political integration hardly had any impact on the growth of the Hong Kong economy. However, the economic integration has raised HK’s annual real GDP by about 4\%.},
	language = {en},
	number = {5},
	urldate = {2020-07-13},
	journal = {Journal of Applied Econometrics},
	author = {Hsiao, Cheng and Steve Ching, H. and Ki Wan, Shui},
	month = aug,
	year = {2012},
	pages = {705--740},
	file = {Hsiao et al. - 2012 - A PANEL DATA APPROACH FOR PROGRAM EVALUATION MEAS.pdf:/Users/dannyklinenberg/Zotero/storage/WA9QHB5U/Hsiao et al. - 2012 - A PANEL DATA APPROACH FOR PROGRAM EVALUATION MEAS.pdf:application/pdf}
}

@article{ferman_synthetic_2019,
	title = {Synthetic {Controls} with {Imperfect} {Pre}-{Treatment} {Fit}},
	url = {http://arxiv.org/abs/1911.08521},
	abstract = {We analyze the properties of the Synthetic Control (SC) and related estimators when the pretreatment ﬁt is imperfect. In this framework, we show that these estimators are generally biased if treatment assignment is correlated with unobserved confounders, even when the number of pretreatment periods goes to inﬁnity. Still, we also show that a modiﬁed version of the SC method can substantially improve in terms of bias and variance relative to the diﬀerence-in-diﬀerence estimator. We also consider the properties of these estimators in settings with non-stationary common factors.},
	language = {en},
	urldate = {2020-07-13},
	journal = {arXiv:1911.08521 [econ]},
	author = {Ferman, Bruno and Pinto, Cristine},
	month = nov,
	year = {2019},
	note = {arXiv: 1911.08521},
	keywords = {Economics - Econometrics},
	file = {Ferman and Pinto - 2019 - Synthetic Controls with Imperfect Pre-Treatment Fi.pdf:/Users/dannyklinenberg/Zotero/storage/DB3B7WRP/Ferman and Pinto - 2019 - Synthetic Controls with Imperfect Pre-Treatment Fi.pdf:application/pdf}
}

@article{ben-michael_augmented_2019,
	title = {The {Augmented} {Synthetic} {Control} {Method}},
	url = {http://arxiv.org/abs/1811.04170},
	abstract = {The synthetic control method (SCM) is a popular approach for estimating the impact of a treatment on a single unit in panel data settings. The “synthetic control” is a weighted average of control units that balances the treated unit’s pre-treatment outcomes as closely as possible. A critical feature of the original proposal is to use SCM only when the ﬁt on pre-treatment outcomes is excellent. We propose Augmented SCM to extend SCM to settings where such pre-treatment ﬁt is infeasible. Analogous to bias correction for inexact matching, Augmented SCM uses an outcome model to estimate the bias due to imperfect pre-treatment ﬁt and then de-biases the original SCM estimate. Our main proposal, which uses ridge regression as the outcome model, directly controls pre-treatment ﬁt while minimizing extrapolation from the convex hull. We bound the bias of this approach under a linear factor model and show how regularization helps to avoid over-ﬁtting to noise. We demonstrate gains from Augmented SCM with extensive simulation studies and apply this framework to estimate the impact of the 2012 Kansas tax cuts on economic growth. We implement the proposed method in the new augsynth R package.},
	language = {en},
	urldate = {2020-07-13},
	journal = {arXiv:1811.04170 [econ, stat]},
	author = {Ben-Michael, Eli and Feller, Avi and Rothstein, Jesse},
	month = nov,
	year = {2019},
	note = {arXiv: 1811.04170},
	keywords = {Economics - Econometrics, Statistics - Methodology},
	file = {Ben-Michael et al. - 2019 - The Augmented Synthetic Control Method.pdf:/Users/dannyklinenberg/Zotero/storage/WP9UMGGC/Ben-Michael et al. - 2019 - The Augmented Synthetic Control Method.pdf:application/pdf}
}

@article{arkhangelsky_synthetic_2019,
	title = {Synthetic {Difference} in {Differences}},
	url = {http://arxiv.org/abs/1812.09970},
	abstract = {We present a new perspective on the Synthetic Control (SC) method as a weighted least squares regression estimator with time ﬁxed eﬀects and unit weights. This perspective suggests a generalization with two way (both unit and time) ﬁxed eﬀects, and both unit and time weights, which can be interpreted as a unit and time weighted version of the standard Diﬀerence In Diﬀerences (DID) estimator. We ﬁnd that this new Synthetic Diﬀerence In Diﬀerences (SDID) estimator has attractive properties compared to the SC and DID estimators. Formally we show that our approach has double robustness properties: the SDID estimator is consistent under a wide variety of weighting schemes given a well-speciﬁed ﬁxed eﬀects model, and SDID is consistent with appropriately penalized SC weights when the basic ﬁxed eﬀects model is misspeciﬁed and instead the true data generating process involves a more general low-rank structure (e.g., a latent factor model). We also present results that justify standard inference based on weighted DID regression. Further generalizations include unit and time weighted factor models.},
	language = {en},
	urldate = {2020-07-13},
	journal = {arXiv:1812.09970 [stat]},
	author = {Arkhangelsky, Dmitry and Athey, Susan and Hirshberg, David A. and Imbens, Guido W. and Wager, Stefan},
	month = jan,
	year = {2019},
	note = {arXiv: 1812.09970},
	keywords = {Statistics - Methodology},
	file = {Arkhangelsky et al. - 2019 - Synthetic Difference in Differences.pdf:/Users/dannyklinenberg/Zotero/storage/Q2YQ7GBR/Arkhangelsky et al. - 2019 - Synthetic Difference in Differences.pdf:application/pdf}
}

@article{gobillon_regional_2016,
	title = {Regional {Policy} {Evaluation}: {Interactive} {Fixed} {Effects} and {Synthetic} {Controls}},
	volume = {98},
	issn = {0034-6535, 1530-9142},
	shorttitle = {Regional {Policy} {Evaluation}},
	url = {http://www.mitpressjournals.org/doi/10.1162/REST_a_00537},
	doi = {10.1162/REST_a_00537},
	abstract = {In this paper, we investigate the use of interactive effect or linear factor models in regional policy evaluation. We contrast treatment effect estimates obtained using Bai (2009) with those obtained using difference in differences and synthetic controls (Abadie and coauthors). We show that difference in differences are generically biased, and we derive support conditions for synthetic controls. We construct Monte Carlo experiments to compare these estimation methods in small samples. As an empirical illustration, we provide an evaluation of the impact on local unemployment of an enterprise zone policy implemented in France in the 1990s.},
	language = {en},
	number = {3},
	urldate = {2020-07-13},
	journal = {Review of Economics and Statistics},
	author = {Gobillon, Laurent and Magnac, Thierry},
	month = jul,
	year = {2016},
	pages = {535--551},
	file = {Gobillon and Magnac - 2016 - Regional Policy Evaluation Interactive Fixed Effe.pdf:/Users/dannyklinenberg/Zotero/storage/EL2VIRLI/Gobillon and Magnac - 2016 - Regional Policy Evaluation Interactive Fixed Effe.pdf:application/pdf}
}

@article{athey_matrix_2020,
	title = {Matrix {Completion} {Methods} for {Causal} {Panel} {Data} {Models}},
	url = {http://arxiv.org/abs/1710.10251},
	abstract = {In this paper we study methods for estimating causal eﬀects in settings with panel data, where a subset of units are exposed to a treatment during a subset of periods, and the goal is estimating counterfactual (untreated) outcomes for the treated unit/period combinations. We develop a class of matrix completion estimators that uses the observed elements of the matrix of control outcomes corresponding to untreated unit/periods to predict the “missing” elements of the matrix, corresponding to treated units/periods. The approach estimates a matrix that well-approximates the original (incomplete) matrix, but has lower complexity according to the nuclear norm for matrices. From a technical perspective, we generalize results from the matrix completion literature by allowing the patterns of missing data to have a time series dependency structure. We also present novel insights concerning the connections between the matrix completion literature, the literature on interactive ﬁxed eﬀects models and the literatures on program evaluation under unconfoundedness and synthetic control methods.},
	language = {en},
	urldate = {2020-07-13},
	journal = {arXiv:1710.10251 [econ, math, stat]},
	author = {Athey, Susan and Bayati, Mohsen and Doudchenko, Nikolay and Imbens, Guido and Khosravi, Khashayar},
	month = jun,
	year = {2020},
	note = {arXiv: 1710.10251},
	keywords = {Economics - Econometrics, Mathematics - Statistics Theory},
	file = {Athey et al. - 2020 - Matrix Completion Methods for Causal Panel Data Mo.pdf:/Users/dannyklinenberg/Zotero/storage/3JCDIAJM/Athey et al. - 2020 - Matrix Completion Methods for Causal Panel Data Mo.pdf:application/pdf}
}

@article{lhour_penalized_2019,
	title = {A {Penalized} {Synthetic} {Control} {Estimator} for {Disaggregated} {Data}},
	abstract = {Synthetic control methods are commonly applied in empirical research to estimate the eﬀects of treatments or interventions of interest on aggregate outcomes. A synthetic control estimator compares the outcome of a treated unit to the outcome of a weighted average of untreated units that best resembles the characteristics of the treated unit before the intervention. When disaggregated data are available, constructing separate synthetic controls for each treated unit may help avoid interpolation biases. However, the problem of ﬁnding a synthetic control that best reproduces the characteristics of a treated unit may not have a unique solution. Multiplicity of solutions is a particularly daunting challenge in settings with disaggregated data, that is, when the sample includes many treated and untreated units. To address this challenge, we propose a synthetic control estimator that penalizes the pairwise discrepancies between the characteristics of the treated units and the characteristics of the units that contribute to their synthetic controls. The penalization parameter trades oﬀ pairwise matching discrepancies with respect to the characteristics of each unit in the synthetic control against matching discrepancies with respect to the characteristics of the synthetic control unit as a whole. We study the properties of this estimator and propose data driven choices of the penalization parameter.},
	language = {en},
	author = {L’Hour, Alberto Abadie Jeremy},
	year = {2019},
	pages = {53},
	file = {L’Hour - A Penalized Synthetic Control Estimator for Disagg.pdf:/Users/dannyklinenberg/Zotero/storage/QCE3FMZ7/L’Hour - A Penalized Synthetic Control Estimator for Disagg.pdf:application/pdf}
}

@article{pang_bayesian_2020,
	title = {Bayesian {Predictive} {Synthesis} {For} {Causal} {Inference} {With} {TSCS} {Data}: {A} {Multilevel} {State}-{Space} {Factor} {Model} {With} {Hierarchical} {Shrinkage} {Priors}},
	language = {en},
	author = {Pang, Xun and Liu, Licheng and Xu, Yiqing},
	year = {2020},
	pages = {54},
	file = {Pang et al. - Bayesian Predictive Synthesis For Causal Inference.pdf:/Users/dannyklinenberg/Zotero/storage/IC7MZLDB/Pang et al. - Bayesian Predictive Synthesis For Causal Inference.pdf:application/pdf;abadie germany.pdf:/Users/dannyklinenberg/Zotero/storage/C85277UT/abadie germany.pdf:application/pdf}
}

@article{abadie_comparative_2015,
	title = {Comparative {Politics} and the {Synthetic} {Control} {Method}: {COMPARATIVE} {POLITICS} {AND} {THE} {SYNTHETIC} {CONTROL} {METHOD}},
	volume = {59},
	issn = {00925853},
	shorttitle = {Comparative {Politics} and the {Synthetic} {Control} {Method}},
	url = {http://doi.wiley.com/10.1111/ajps.12116},
	doi = {10.1111/ajps.12116},
	language = {en},
	number = {2},
	urldate = {2020-07-13},
	journal = {American Journal of Political Science},
	author = {Abadie, Alberto and Diamond, Alexis and Hainmueller, Jens},
	month = feb,
	year = {2015},
	pages = {495--510},
	file = {Abadie et al. - 2015 - Comparative Politics and the Synthetic Control Met.pdf:/Users/dannyklinenberg/Zotero/storage/6AXPL83N/Abadie et al. - 2015 - Comparative Politics and the Synthetic Control Met.pdf:application/pdf}
}

@article{fujiki_disentangling_2015,
	title = {Disentangling the effects of multiple treatments—{Measuring} the net economic impact of the 1995 great {Hanshin}-{Awaji} earthquake},
	volume = {186},
	issn = {03044076},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304407614002541},
	doi = {10.1016/j.jeconom.2014.10.010},
	abstract = {We propose a panel data approach to disentangle the impact of ‘‘one treatment’’ from the ‘‘other treatment’’ when the observed outcomes are subject to both treatments. We use the Great Hanshin-Awaji earthquake that took place on January 17, 1995 to illustrate our methodology. We find that there were no persistent earthquake effects. The observed persistent effects are due to structural change in Hyogo prefecture.},
	language = {en},
	number = {1},
	urldate = {2020-07-13},
	journal = {Journal of Econometrics},
	author = {Fujiki, Hiroshi and Hsiao, Cheng},
	month = may,
	year = {2015},
	pages = {66--73},
	file = {Fujiki and Hsiao - 2015 - Disentangling the effects of multiple treatments—M.pdf:/Users/dannyklinenberg/Zotero/storage/V9CGULUL/Fujiki and Hsiao - 2015 - Disentangling the effects of multiple treatments—M.pdf:application/pdf}
}

@article{cavallo_catastrophic_2013,
	title = {Catastrophic {Natural} {Disasters} and {Economic} {Growth}},
	abstract = {We examine the average causal impact of catastrophic natural disasters on economic growth by combining information from comparative case studies. For each country affected by a large disaster, we compute the counterfactual by constructing synthetic controls. We ﬁnd that only extremely large disasters have a negative effect on output in both the short and the long runs. However, we also show that this results from two events where radical political revolutions followed the disasters. Once we control for these political changes, even extremely large disasters do not display any signiﬁcant effect on economic growth.},
	language = {en},
	journal = {THE REVIEW OF ECONOMICS AND STATISTICS},
	author = {Cavallo, Eduardo and Galiani, Sebastian and Noy, Ilan and Pantano, Juan},
	year = {2013},
	pages = {13},
	file = {Cavallo et al. - CATASTROPHIC NATURAL DISASTERS AND ECONOMIC GROWTH.pdf:/Users/dannyklinenberg/Zotero/storage/9GDJV33F/Cavallo et al. - CATASTROPHIC NATURAL DISASTERS AND ECONOMIC GROWTH.pdf:application/pdf}
}

@article{grossman_impact_2019,
	title = {The {Impact} of the {Flint} {Water} {Crisis} on {Fertility}},
	volume = {56},
	issn = {0070-3370, 1533-7790},
	url = {http://link.springer.com/10.1007/s13524-019-00831-0},
	doi = {10.1007/s13524-019-00831-0},
	abstract = {Flint switched its public water source in April 2014, increasing exposure to lead and other contaminants. We compare the change in the fertility rate and in health at birth in Flint before and after the water switch to the changes in other cities in Michigan. We find that Flint fertility rates decreased by 12 \% and that overall health at birth decreased. This effect on health at birth is a function of two countervailing mechanisms: (1) negative selection of less healthy embryos and fetuses not surviving (raising the average health of survivors), and (2) those who survived being scarred (decreasing average health). We untangle this to find a net of selection scarring effect of 5.4 \% decrease in birth weight. Because of long-term effects of in utero exposure, these effects are likely lower bounds on the overall effects of this exposure.},
	language = {en},
	number = {6},
	urldate = {2020-07-13},
	journal = {Demography},
	author = {Grossman, Daniel S. and Slusky, David J.G.},
	month = dec,
	year = {2019},
	pages = {2005--2031},
	file = {Grossman and Slusky - 2019 - The Impact of the Flint Water Crisis on Fertility.pdf:/Users/dannyklinenberg/Zotero/storage/JM7NN6X8/Grossman and Slusky - 2019 - The Impact of the Flint Water Crisis on Fertility.pdf:application/pdf}
}

@article{aytug_twenty_2017,
	title = {Twenty {Years} of the {EU}-{Turkey} {Customs} {Union}: {A} {Synthetic} {Control} {Method} {Analysis}: {Twenty} {Years} of the {EU}-{Turkey} {Customs} {Union}: {Effects} of {EU} {Integration}},
	volume = {55},
	issn = {00219886},
	shorttitle = {Twenty {Years} of the {EU}-{Turkey} {Customs} {Union}},
	url = {http://doi.wiley.com/10.1111/jcms.12490},
	doi = {10.1111/jcms.12490},
	abstract = {The paper studying the 1995 EU–Turkey Customs Union (CU) delivers a quantitative assessment of trade and GDP per capita effects of the CU on the Turkish economy. Our Synthetic Control Method based analysis reveals, contrary to the results of most studies in the literature, that the CU’s effects have been substantial by any standards. In particular, the paper shows that in the absence of the EU–Turkey CU, Turkish exports to the EU and GDP per capita would have been 38 per cent and 13 per cent less, respectively.},
	language = {en},
	number = {3},
	urldate = {2020-07-13},
	journal = {JCMS: Journal of Common Market Studies},
	author = {Aytuğ, Hüseyin and Kütük, Merve Mavuş and Oduncu, Arif and Togan, Sübidey},
	month = may,
	year = {2017},
	pages = {419--431},
	file = {Aytuğ et al. - 2017 - Twenty Years of the EU-Turkey Customs Union A Syn.pdf:/Users/dannyklinenberg/Zotero/storage/RBLMUIV5/Aytuğ et al. - 2017 - Twenty Years of the EU-Turkey Customs Union A Syn.pdf:application/pdf}
}

@article{born_costs_2019,
	title = {The {Costs} of {Economic} {Nationalism}: {Evidence} from the {Brexit} {Experiment}*},
	volume = {129},
	issn = {0013-0133, 1468-0297},
	shorttitle = {The {Costs} of {Economic} {Nationalism}},
	url = {https://academic.oup.com/ej/article/129/623/2722/5506774},
	doi = {10.1093/ej/uez020},
	abstract = {Abstract
            Economic nationalism is on the rise, but at what cost? We study this question using the unexpected outcome of the Brexit referendum vote as a natural macroeconomic experiment. Employing synthetic control methods, we first show that the Brexit vote has caused a UK output loss of 1.7\% to 2.5\% by year-end 2018. An expectations-augmented VAR suggests that these costs are, to a large extent, driven by a downward revision of growth expectations in response to the vote. Linking quasi-experimental identification to structural time-series estimation allows us not only to quantify the aggregate costs but also to understand the channels through which expected economic disintegration impacts the macroeconomy.},
	language = {en},
	number = {623},
	urldate = {2020-07-13},
	journal = {The Economic Journal},
	author = {Born, Benjamin and Müller, Gernot J and Schularick, Moritz and Sedláček, Petr},
	month = oct,
	year = {2019},
	pages = {2722--2744},
	file = {Born et al. - 2019 - The Costs of Economic Nationalism Evidence from t.pdf:/Users/dannyklinenberg/Zotero/storage/UQSBH5DS/Born et al. - 2019 - The Costs of Economic Nationalism Evidence from t.pdf:application/pdf}
}

@article{dorsett_effect_2013,
	title = {The effect of the {Troubles} on {GDP} in {Northern} {Ireland}},
	volume = {29},
	issn = {01762680},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0176268012000638},
	doi = {10.1016/j.ejpoleco.2012.10.003},
	abstract = {This paper explores the effect of conflict on GDP in Northern Ireland. A synthetic control region constructed as a weighted average of other UK regions provides an estimate of counterfactual ‘no-conflict’ GDP. Comparing this with actual per capita GDP suggests a negative impact of up to 10\%. Excluding the increased grants provided in response to the conflict, a 15–20\% reduction is evident. Most forms of terrorist activity had negative effects over the period 1969–1997. Deaths attributable to Republican paramilitary groups or to the State appear to have a greater and more lasting impact on GDP than deaths attributable to Loyalist paramilitaries.},
	language = {en},
	urldate = {2020-07-13},
	journal = {European Journal of Political Economy},
	author = {Dorsett, Richard},
	month = mar,
	year = {2013},
	pages = {119--133},
	file = {Dorsett - 2013 - The effect of the Troubles on GDP in Northern Irel.pdf:/Users/dannyklinenberg/Zotero/storage/VEYJYG89/Dorsett - 2013 - The effect of the Troubles on GDP in Northern Irel.pdf:application/pdf}
}

@article{bilgel_economic_2017,
	title = {The {Economic} {Costs} of {Separatist} {Terrorism} in {Turkey}},
	volume = {61},
	issn = {0022-0027, 1552-8766},
	url = {http://journals.sagepub.com/doi/10.1177/0022002715576572},
	doi = {10.1177/0022002715576572},
	abstract = {Turkey has been suffering from separatist terrorism and the political conflict it implies since the mid-1980s, both of which are believed to have a negative impact on economic welfare. This article investigates the economic costs of Kurdistan Workers’ Party (PKK) terrorism, particularly in the Eastern and Southeastern provinces of Turkey by invoking the synthetic control method. We create a synthetic control group that mimics the socioeconomic characteristics of the provinces exposed to terrorism before the PKK terrorism emerged in the mid-1980s. We then compare the real gross domestic product (GDP) of the synthetic provinces without terrorism to the actual provinces with terrorism for the period 1975 to 2001. Causal inference is carried out by comparing the real per capita GDP gap between the synthetic and actual provinces against the intensity of PKK terrorist activity. Extended over a period of fourteen years (1988 to 2001), we find that after the emergence of terrorism, the per capita real GDP in Eastern and Southeastern Anatolia declined by about 6.6 percent relative to a comparable synthetic Eastern and Southeastern Anatolia without terrorism.},
	language = {en},
	number = {2},
	urldate = {2020-07-13},
	journal = {Journal of Conflict Resolution},
	author = {Bilgel, Fırat and Karahasan, Burhan Can},
	month = feb,
	year = {2017},
	pages = {457--479},
	file = {Bilgel and Karahasan - 2017 - The Economic Costs of Separatist Terrorism in Turk.pdf:/Users/dannyklinenberg/Zotero/storage/S9B6XGEG/Bilgel and Karahasan - 2017 - The Economic Costs of Separatist Terrorism in Turk.pdf:application/pdf}
}

@article{cunningham_decriminalizing_nodate,
	title = {Decriminalizing {Indoor} {Prostitution}: {Implications} for {Sexual} {Violence} and {Public} {Health}},
	abstract = {Most governments in the world including the United States prohibit prostitution. Given these types of laws rarely change and are fairly uniform across regions, our knowledge about the impact of decriminalizing sex work is largely conjectural. We exploit the fact that a Rhode Island District Court judge unexpectedly decriminalized indoor prostitution in 2003 to provide the first causal estimates of the impact of decriminalization on the composition of the sex market, rape offenses, and sexually transmitted infection outcomes. Not surprisingly, we find that decriminalization increased the size of the indoor market. However, we also find that decriminalization caused both forcible rape offenses and gonorrhea incidence to decline for the overall population. Our synthetic control model finds 824 fewer reported rape offenses (31 percent decrease) and 1,035 fewer cases of female gonorrhea (39 percent decrease) from 2004 to 2009.},
	language = {en},
	author = {Cunningham, Scott and Shah, Manisha},
	pages = {55},
	file = {Cunningham and Shah - Decriminalizing Indoor Prostitution Implications .pdf:/Users/dannyklinenberg/Zotero/storage/5P7BEHM8/Cunningham and Shah - Decriminalizing Indoor Prostitution Implications .pdf:application/pdf}
}

@article{cunningham_fracking_2020,
	title = {Fracking and risky sexual activity},
	volume = {72},
	issn = {01676296},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167629619308513},
	doi = {10.1016/j.jhealeco.2020.102322},
	abstract = {Theoretical links between sex ratio imbalance and risky sexual activity are wellestablished, but empirically establishing causal links remains elusive. We use the fracking boom as a plausibly exogenous shifter of sex ratios to evaluate eﬀects on STI transmission and prostitution. We ﬁrst document positive overall eﬀects on Gonorrhea rates in fracking counties, but no eﬀects on prostitution. We then perform case studies focusing on remote, high-production areas that experienced signiﬁcant male in-migration. Using OLS and synthetic controls, we ﬁnd large eﬀects on Gonorrhea and prostitution in North Dakota. For other remote, high-production areas we observe moderate evidence for both outcomes.},
	language = {en},
	urldate = {2020-07-13},
	journal = {Journal of Health Economics},
	author = {Cunningham, Scott and DeAngelo, Gregory and Smith, Brock},
	month = jul,
	year = {2020},
	pages = {102322},
	file = {Cunningham et al. - 2020 - Fracking and risky sexual activity.pdf:/Users/dannyklinenberg/Zotero/storage/I6N3HC64/Cunningham et al. - 2020 - Fracking and risky sexual activity.pdf:application/pdf}
}

@article{rubin_formal_1990,
	title = {Formal mode of statistical inference for causal effects},
	volume = {25},
	issn = {03783758},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0378375890900778},
	doi = {10.1016/0378-3758(90)90077-8},
	language = {en},
	number = {3},
	urldate = {2020-07-14},
	journal = {Journal of Statistical Planning and Inference},
	author = {Rubin, Donald B.},
	month = jul,
	year = {1990},
	pages = {279--292}
}

@article{pang_modeling_2010,
	title = {Modeling {Heterogeneity} and {Serial} {Correlation} in {Binary} {Time}-{Series} {Cross}-sectional {Data}: {A} {Bayesian} {Multilevel} {Model} with {AR}(p) {Errors}},
	volume = {18},
	issn = {1047-1987, 1476-4989},
	shorttitle = {Modeling {Heterogeneity} and {Serial} {Correlation} in {Binary} {Time}-{Series} {Cross}-sectional {Data}},
	url = {https://www.cambridge.org/core/product/identifier/S1047198700012572/type/journal_article},
	doi = {10.1093/pan/mpq019},
	abstract = {This paper proposes a Bayesian generalized linear multilevel model with a
              p
              th-order autoregressive error process to analyze unbalanced binary time-series cross-sectional (TSCS) data. The model specification is motivated by the generic TSCS data structure and is intended to handle the associated inefficiency and endogeneity problems. It accommodates heterogeneity across units and between time periods in the form of random intercepts and random-effect coefficients. At the same time, its
              p
              th-order autoregressive error process, employed either by itself or in concert with other dynamic methods, adequately corrects serial correlation and improves statistical inference and forecasting. With a stationarity restriction on the error process, the model can also be used as a residual-based cointegration test on discrete TSCS data. This is especially valuable because cointegration testing on discrete TSCS data is methodologically challenging and rarely conducted in practice. To handle the estimation difficulties, I developed an efficient Markov chain Monte Carlo (MCMC) algorithm by orthogonalizing the error term with the Cholesky decomposition and adding an auxiliary variable. The parameter expansion method, that is, partial group move—multigrid Monte Carlo updating (PGM-MGMC), is employed to further improve MCMC mixing and speed up convergence. The paper also provides a computational scheme to approximate the Bayes's factor for the purposes of serial correlation diagnostics, lag order determination, and variable selection. Simulated and empirical examples are used to assess the model and techniques.},
	language = {en},
	number = {4},
	urldate = {2020-07-15},
	journal = {Political Analysis},
	author = {Pang, Xun},
	year = {2010},
	pages = {470--498},
	file = {Pang - 2010 - Modeling Heterogeneity and Serial Correlation in B.pdf:/Users/dannyklinenberg/Zotero/storage/DM8GTL7Z/Pang - 2010 - Modeling Heterogeneity and Serial Correlation in B.pdf:application/pdf}
}

@article{cattaneo_prediction_2019,
	title = {Prediction {Intervals} for {Synthetic} {Control} {Methods}},
	url = {http://arxiv.org/abs/1912.07120},
	abstract = {Uncertainty quantiﬁcation is a fundamental problem in the analysis and interpretation of synthetic control (SC) methods. We develop prediction intervals in the canonical SC framework, and provide conditions under which these intervals oﬀer ﬁnite-sample probability guarantees. Our construction begins by noting that the statistical uncertainty of the SC prediction is governed by two distinct sources of randomness: one coming from the construction of the (likely misspeciﬁed) SC weights in the pre-treatment period, and the other coming from the unobservable stochastic error in the post-treatment period when the treatment eﬀect is analyzed. Accordingly, our proposed prediction intervals are constructed taking into account both sources of randomness. For implementation, we propose a multiplier bootstrap approach along with ﬁnite-sample-based probability bound arguments. We illustrate the performance of our proposed prediction intervals in the context of three empirical applications from the SC literature.},
	language = {en},
	urldate = {2020-07-15},
	journal = {arXiv:1912.07120 [econ, stat]},
	author = {Cattaneo, Matias D. and Feng, Yingjie and Titiunik, Rocio},
	month = dec,
	year = {2019},
	note = {arXiv: 1912.07120},
	keywords = {Economics - Econometrics, Statistics - Methodology},
	file = {Cattaneo et al. - 2019 - Prediction Intervals for Synthetic Control Methods.pdf:/Users/dannyklinenberg/Zotero/storage/GRHZJXJA/Cattaneo et al. - 2019 - Prediction Intervals for Synthetic Control Methods.pdf:application/pdf}
}

@article{gutman_bayesian_2018,
	title = {A {Bayesian} procedure for estimating the causal effects of nursing home bed-hold policy},
	volume = {19},
	issn = {1465-4644, 1468-4357},
	url = {https://academic.oup.com/biostatistics/article/19/4/444/4321718},
	doi = {10.1093/biostatistics/kxx049},
	abstract = {Nursing home bed-hold policies provide continuity of care for Medicaid beneﬁciaries by paying nursing homes to reserve beds so residents can return to their facility of occupancy following an acute hospitalization. In 2001, Michigan implemented bed-hold policies in nursing homes. We investigated the impact of these policies on mortality and hospitalizations using 1999–2004 quarterly data from nursing homes in Michigan and nursing homes in 11 states that did not implement such policies. Synthetic Control has been used to estimate the effects of policies by accounting for changes over time unrelated to the intervention. Synthetic Control is intended for scalar continuous outcome at each period, and assumes a single treated unit and multiple control units. We propose a Bayesian procedure to overcome these limitations. It imputes the outcomes of nursing homes in Michigan if they were not exposed to the policy by matching to non-exposed nursing homes that are associated with the exposed ones in the pre-policy period. Because sampling from a Bayesian model is computationally challenging, we describe an approximation procedure that can be implemented using existing software. Our approach can be applied to other studies that examine the impact of policies.},
	language = {en},
	number = {4},
	urldate = {2020-07-16},
	journal = {Biostatistics},
	author = {Gutman, Roee and Intrator, Orna and Lancaster, Tony},
	month = oct,
	year = {2018},
	pages = {444--460},
	file = {Gutman et al. - 2018 - A Bayesian procedure for estimating the causal eff.pdf:/Users/dannyklinenberg/Zotero/storage/53LSJ46K/Gutman et al. - 2018 - A Bayesian procedure for estimating the causal eff.pdf:application/pdf}
}

@article{athey_ensemble_2019,
	title = {Ensemble {Methods} for {Causal} {Effects} in {Panel} {Data} {Settings}},
	volume = {109},
	issn = {2574-0768, 2574-0776},
	url = {https://pubs.aeaweb.org/doi/10.1257/pandp.20191069},
	doi = {10.1257/pandp.20191069},
	abstract = {In many prediction problems researchers have found that combinations of prediction methods (“ensembles”) perform better than individual methods. In this paper we apply these ideas to synthetic control type problems in panel data. Here a number of conceptually quite different methods have been developed. We compare the predictive accuracy of three methods with an ensemble method and find that the latter dominates. These results show that ensemble methods are a practical and effective method for the type of data configurations typically encountered in empirical work in economics, and that these methods deserve more attention.},
	language = {en},
	urldate = {2020-07-30},
	journal = {AEA Papers and Proceedings},
	author = {Athey, Susan and Bayati, Mohsen and Imbens, Guido and Qu, Zhaonan},
	month = may,
	year = {2019},
	pages = {65--70},
	file = {Athey et al. - 2019 - Ensemble Methods for Causal Effects in Panel Data .pdf:/Users/dannyklinenberg/Zotero/storage/EL9KK3H5/Athey et al. - 2019 - Ensemble Methods for Causal Effects in Panel Data .pdf:application/pdf}
}

@article{kellogg_combining_nodate,
	title = {Combining {Matching} and {Synthetic} {Control} to {Trade} off {Biases} from {Extrapolation} and {Interpolation}},
	abstract = {The synthetic control method is widely used in comparative case studies to adjust for diﬀerences in pre-treatment characteristics. A major attraction of the method is that it limits extrapolation bias that can occur when untreated units with diﬀerent pretreatment characteristics are combined using a traditional adjustment, such as a linear regression. Instead, the SC estimator is susceptible to interpolation bias because it uses a convex weighted average of the untreated units to create a synthetic untreated unit with pre-treatment characteristics similar to those of the treated unit. More traditional matching estimators exhibit the opposite behavior: They limit interpolation bias at the potential expense of extrapolation bias. We propose combining the matching and synthetic control estimators through model averaging. We show how to use a rollingorigin cross-validation procedure to train the model averaging estimator to resolve trade-oﬀs between interpolation and extrapolation bias. We evaluate the estimator through Monte Carlo simulations and placebo studies before using it to re-examine the economic costs of conﬂicts. Not only does the model averaging estimator perform far better than synthetic controls and other alternatives in the simulations and placebo exercises. It also yields treatment eﬀect estimates that are substantially diﬀerent from the other estimators.},
	language = {en},
	author = {Kellogg, Maxwell and Pouliot, Guillaume A and Mogstad, Magne and Torgovitsky, Alexander},
	pages = {28},
	file = {Kellogg et al. - Combining Matching and Synthetic Control to Trade .pdf:/Users/dannyklinenberg/Zotero/storage/55KQ6UW7/Kellogg et al. - Combining Matching and Synthetic Control to Trade .pdf:application/pdf}
}

@article{carvalho_perils_2016,
	title = {The {Perils} of {Counterfactual} {Analysis} with {Integrated} {Processes}},
	issn = {1556-5068},
	url = {http://www.ssrn.com/abstract=2894065},
	doi = {10.2139/ssrn.2894065},
	abstract = {Recently, there has been a growing interest in developing econometric tools to conduct counterfactual analysis with aggregate data when a “treated” unit suﬀers an intervention, such as a policy change, and there is no obvious control group. Usually, the proposed methods are based on the construction of an artiﬁcial counterfactual from a pool of “untreated” peers, organized in a panel data structure. In this paper, we investigate the consequences of applying such methodologies when the data are formed by integrated process of order 1. We ﬁnd that without a cointegration relation (spurious case) the intervention estimator diverges resulting in the rejection of the hypothesis of no intervention eﬀect regardless of its existenc√e. Whereas, for the case when at least one cointegration relation exists, we have a T -consistent estimator for the intervention eﬀect albeit with a non-standard distribution. However, even in this case, the test of no intervention eﬀect is extremely oversized if nonstationarity is ignored. When a drift is present in the data generating processes, the estimator for both cases (cointegrated and spurious) either diverges or is not well deﬁned asymptotically. As a ﬁnal recommendation we suggest to work in ﬁrst-diﬀerences to avoid spurious results.},
	language = {en},
	urldate = {2020-09-24},
	journal = {SSRN Electronic Journal},
	author = {Carvalho, Carlos},
	year = {2016},
	file = {Carvalho - 2016 - The Perils of Counterfactual Analysis with Integra.pdf:/Users/dannyklinenberg/Zotero/storage/4YWLHSDM/Carvalho - 2016 - The Perils of Counterfactual Analysis with Integra.pdf:application/pdf}
}

@article{anyanwu,
	title = {Why Does Foreign Direct Investment Go Where It Goes?: New Evidence From African Countries},
	author = {{Anyanwu}, {John C}},
	pages = {38},
	langid = {en}
}

@article{abadie2010,
	title = {Synthetic Control Methods for Comparative Case Studies: Estimating the Effect of California{\textquoteright}s Tobacco Control Program},
	author = {{Abadie}, {Alberto} and {Diamond}, {Alexis} and {Hainmueller}, {Jens}},
	year = {2010},
	month = {06},
	date = {2010-06},
	journal = {Journal of the American Statistical Association},
	pages = {493--505},
	volume = {105},
	number = {490},
	doi = {10.1198/jasa.2009.ap08746},
	url = {http://www.tandfonline.com/doi/abs/10.1198/jasa.2009.ap08746},
	langid = {en}
}

@article{abadie2010a,
	title = {Synthetic Control Methods for Comparative Case Studies: Estimating the Effect of California{\textquoteright}s Tobacco Control Program},
	author = {{Abadie}, {Alberto} and {Diamond}, {Alexis} and {Hainmueller}, {Jens}},
	year = {2010},
	month = {06},
	date = {2010-06},
	journal = {Journal of the American Statistical Association},
	pages = {493--505},
	volume = {105},
	number = {490},
	doi = {10.1198/jasa.2009.ap08746},
	url = {http://www.tandfonline.com/doi/abs/10.1198/jasa.2009.ap08746},
	langid = {en}
}
