---
title: "par_est"
output: pdf_document
---
# Estimation of Parameters and Counterfactual

I estimate the parameters using a Bayesian approach. $g_{T_0+i}\left(y_{0,1},\dots,y_{0,T_0}\right)$ will be estimated as a conditional distribution on the observable:

\begin{align}
g_{T_0+i}\left(y_{0,1},\dots,y_{0,T_0}\right) \sim f_{T_0+i}(y_{0,T_0+i}|\mathbf{y},\mathbf{y_0})
\end{align}

This distribution can be rewritten as a model average on parameters:

\begin{align}
f_{T_0+i}(y_{0,T_0+i}|\mathbf{y},\mathbf{y_0}) = \int_{v \in V} f_{T_0+i}(y_{0,T_0+i}|\mathbf{y},\mathbf{y_0},v)Pr(v|\mathbf{y},\mathbf{y_0})dv
\end{align}

Given the state space setup, $f_{T_0+i}(y_{0,T_0+i}|\mathbf{y},\mathbf{y_0},v) \sim \mathcal{N}\left(\sum_{j=1}^{J+1} \beta_{j,T_0+i}y_{j,T_0+i},\sigma^2\right)$. To estimate the posterior parameters, I incorporate a variant of the Bayesian Lasso [@park_bayesian_2008]. I model the shrinkage using the global-local shrinkage framework [@bernardo_shrink_2011]. This framework is designed to apply stronger amounts of shrinkage for smaller parameter estimates allowing larger parameter estimates to "escape" leading to better in-sample and out-of-sample fits.

## Reparameterization

Equation \eqref{eq:rw} can be rewritten to decompose $\beta_{j,t}$ into a time varying and constant components [^further]:

[^further]: See @fruhwirth-schnatter_stochastic_2010 for more details.

$$
\begin{aligned}
\beta_{j,t}&=\beta_{j}+\tilde{\beta}_{j,t}\sqrt{\theta_j}\\
\tilde{\beta}_{j,t}&= \tilde{\beta}_{j,t-1}+\tilde{\eta}_{j,t} & \tilde{\eta}_{j,t} \sim N(0,1)\\
\tilde{\beta}_{j,0}& \sim N(0,P_{jj})\\
\end{aligned}
$$

with priors provided below. $\beta_j$ can now be interpreted as the time invariant component of $\beta_{j,t}$ and $\sqrt{\theta_j}\tilde{\beta}_{j,t}$ the time varying component. $\sqrt{\theta_j}$ is defined as the root of $\theta_j$ and allowed to take both positive and negative values. Defining $\sqrt{\theta_j}$ in this manner allows 0 to be an interior point in the prior distribution. This is a desirable feature when performing Bayesian shrinkage [@bitto_achieving_2019]. The absolute value of $\sqrt{\theta_j}$ is the standard deviation of the time varying coefficient. Substituting the reformulation back into the original equation yields the proposed state space model.

\begin{assumption}
The Time Varying Parameter Bayesian Lasso ( BL-TVP ) takes the following form:
\end{assumption}


\begin{align}
y_{0,t}(0)&=\sum_{j=1}^{J+1} \left(\beta_{j}+\tilde{\beta}_{j,t}\sqrt{\theta_j}\right)y_{j,t}+\epsilon_t & \epsilon_t \sim N(0, \sigma^2) \label{eq:rmod1}\\
\tilde{\beta}_{j,t}&= \tilde{\beta}_{j,t-1}+\tilde{\eta}_{j,t} & \tilde{\eta}_{j,t} \sim N(0,1) \label{eq:rmod2}\\
\tilde{\beta}_{j,0}& \sim N(0,P_{jj}) \\
\pi(\mathbf{v}) \label{eq:rmod3}
\end{align}


where $\pi(\mathbf{v})$ represents the prior distribution of the parameters with the specific distributions defined below. Equations \eqref{eq:rmod1} - \eqref{eq:rmod3} constitute the model. This setup is commonly known as the *non-centered parameterization of state space models*. This formulation allows estimation of the time varying and time invariant component of the coefficients individually. The effect of each control unit can be summarized into one of the four categories: (i) time varying non-zero, (ii) time invariant, (iii) time varying centered at zero, and (iv) time invariant zero coefficients (irrelevant). 

Notice if $\sqrt{\theta_j}=0$ for all j, the model is a Bayesian version of the LASSO estimator discussed in @kinn_synthetic_2018. Setting $\sqrt{\theta_j}=0$ and restricting $\beta$ such that $\beta_j \in [0,1]$ and $\sum_j \beta_j =1$ yields a parametric version of @abadie_economic_2003 synthetic control model. Similarly, if the data generating process is spurious (e.g. $\beta_j=\sqrt{\theta_j}=0$ for all j), then \eqref{eq:rmod1} - \eqref{eq:rmod3} collapses to a local level model.

There are 2(J+1)+1 parameters to be estimated: the J+1 time invariant coefficients (i.e. $\beta_j$'s), the J+1 time varying coefficients (i.e. $\sqrt{\theta_j}$'s) and the variance ($\sigma^2$).

### Bayesian Shrinkage Priors

I set up the prior distribution for coefficients ${\beta}=[\beta_1,\beta_2,...,\beta_{J+1}]$ with variances $\alpha^2=[\alpha_1^2,\alpha_2^2,...,\alpha_{J+1}^2]$ as a *global-local* shrinkage prior: 

$$
\begin{aligned}
\beta | \alpha^2, \lambda^2 &\sim \mathcal{N}_{J+1} (0_{J+1}, \lambda^2 diag[\alpha_1^2,...\alpha_{J+1}^2])\\
\alpha_j^2  &\sim \pi(\alpha_j^2)\\
\lambda^2 &\sim \pi(\lambda^2)
\end{aligned}
$$

This prior formulation has gained popularity in the Bayesian framework due to it's attractive shrinkage properties (@makalic_high-dimensional_2016, @bernardo_shrink_2011). $\lambda^2$ controls the overall complexity of the model while $\alpha_j^2$ produces individual shrinkage. This formulation allows for strong shrinkage on small coefficients while leaving larger coefficients relatively unshrunk.

$\alpha_j^2$ is assigned an exponential distribution with rate 1. The hierarchical formulation of $\beta$ and $\alpha^2$ are identical to a priori independent Laplace priors. Such a prior forms the Bayesian LASSO proposed by @park_bayesian_2008.  @park_bayesian_2008 showed this choice of priors leads to posterior performance similar to the frequentist machine learning approach LASSO [@tibshirani_regression_1996].

$\lambda^2$ is represented as a half-Cauchy distribution with mean 0 and scale parameter 1. The half-Cauchy is used for the global shrinkage prior because of the flexibility and better behavior near 0 compared to alternatives [@polson_half-cauchy_2011]. In addition, the half-Cauchy has significant amounts of mass at the point 0 leading to better shrinkage properties. 

Like the Laplace distribution, the half-Cauchy has a hierarchical representation where $\lambda^2|\zeta_{\beta}$ follows an inverse gamma with shape parameter 1/2 and rate 1/$\zeta_{\beta}$. The hierarchical parameter, $\zeta_\beta$, follows an inverse gamma with shape parameter 1/2 and rate parameter 1. The prior distribution for $\beta=[\beta_1,\beta_2,...,\beta_{J+1}]$ with variances $\alpha^2=[\alpha_1^2,\alpha_2^2,...,\alpha_{J+1}^2]$ are:


\begin{align}
\beta | \alpha^2, \lambda^2 &\sim \mathcal{N}_{J+1} (0_{J+1}, \lambda^2 diag[\alpha_1^2,...\alpha_{J+1}^2])\\
\alpha_j^2  &\sim exp\left(1\right)\\
\lambda^2 |\zeta_\beta &\sim InverseGamma\left(\frac{1}{2}, \frac{1}{\zeta_\beta}\right)\\
\zeta_{\beta} & \sim InverseGamma\left(\frac{1}{2},1\right)
\end{align}


Traditionally, variances have been defined by the inverse gamma distribution. However, the inverse gamma does not allow for effective shrinkage given its support. @fruhwirth-schnatter_stochastic_2010 provide an in depth argument for the use of the normal distribution as an alternative. Briefly, the inverse gamma prior performs poorly in terms of shrinkage due to 0 being an extreme value in the distribution. This limits the amount of mass which can be placed at 0 in turn limiting the amount of shrinkage. This becomes a problem when there is believed to be many parameters equal to zero. The normal distribution  allows for mass at zero avoiding this problem. Similarly to $\beta$, assign the prior of $\sqrt{\theta}=\left[\sqrt{\theta_1},\sqrt{\theta_2},...,\sqrt{\theta_{J+1}}\right]$ with variances $\xi^2=\left[\xi_1^2,\xi_2^2,...\xi_{J+1}^2 \right]$ as:


\begin{align}
\sqrt{\theta} | \xi^2, \kappa^2 &\sim \mathcal{N}_{J+1} (0_{J+1}, \kappa^2 diag[\xi_1^2,...\xi_{J+1}^2])\\
\xi_j^2  &\sim exp\left(1\right)\\
\kappa^2 |\zeta_{\sqrt{\theta}} &\sim InverseGamma\left(\frac{1}{2}, \frac{1}{\zeta_{\sqrt{\theta}}}\right)\\
\zeta_{{\sqrt{\theta}}} & \sim InverseGamma\left(\frac{1}{2},1\right)
\end{align}


$\sigma^2$ is defined as $\frac{1}{\sigma^2} \sim Gamma(a_1,a_2)$ with *shape* hyperparameter $a_1$ and  *scale* hyperparameter $a_2$. If $\sqrt{\theta_j}=0$ for all j, the model collapses to a time invariant estimation with the Bayesian LASSO performing shrinkage. A derivation of the posterior distributions can be found in the appendix.
