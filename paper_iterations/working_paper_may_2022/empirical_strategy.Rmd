---
title: "Empirical Strategy"
output: pdf_document
linkcolor: blue
bibliography: "references.bib"
link-citations: yes

---

# Empirical Strategy \label{section:strategy}

## Empirical Approach

The goal of this paper is to identify the average causal effect of fraternity moratoriums on alcohol and sexual assault offenses across universities that experience moratoriums. In a naive analysis, this would amount to taking a difference of means for moratorium days and non-moratorium days across all universities for each offense. However, there are several issues with identifying such difference of means as a causal effect. First, university police departments each vary considerably in the frequency of reporting offenses. This is the result of differing policing tactics, the departments' available resources—such as number of officers per-student—and the overall composition of students at the university. For instance, a police department that oversees a university with a reputation for partying may police differently than a police department that rarely encounters college partying. Second, frequencies of offenses vary depending on the day of the week and the time of year. As an example, alcohol offenses most commonly occur on Fridays, Saturdays and Sundays, and fraternity recruitment is typically in the fall semester. A simple difference of means fails to account for each of these systematic differences between universities, days of the week, and semesters. 

To circumvent these issues, I estimate the following baseline difference-in-differences specification using OLS:

\begin{equation}\label{main_model}
Y_{u,t} = \beta Moratorium_{u,t}  + \gamma_{u} + \lambda \mathbb{X}_{t} + \epsilon_{u,t}
\end{equation}

\noindent In Equation \ref{main_model}, $Y_{u,t}$ is an outcome of alcohol offenses or sexual assaults per-25000 enrolled students per academic-calendar day at university $u$ in time $t$. $Moratorium_{u,t}$ is an indicator variable equal to one when university $u$ is undergoing a moratorium at time $t$, $\gamma_u$ is a university-specific fixed effect, $\mathbb{X}_t$ is a vector of time-varying controls that are shared across universities, and $\epsilon_{u,t}$ is the error term. The standard errors are clustered by university [@bertrand_how_2004]. In essence, Equation \ref{main_model} is comparing moratorium days to non-moratorium days within universities that have experienced, or will experience a moratorium while accounting for expected differences across universities and time.

Including university-specific fixed effects ($\gamma_u$) in the baseline model accounts for systematic differences between a university's police department, the corresponding student demographic they are are policing, and overall fixed differences in incident prevalence. As stated above, a police department may have systematic differences in the frequency of reporting due to the corresponding demographic of the university or their own policing practices. For example, some police departments may enforce policies against underage drinking stricter than others. Hence, including university-specific fixed effects ensures that moratorium days are compared to non-moratorium days while adjusting for these expected differences in universities. Moreover, $\mathbb{X}_{t}$ includes day of the week, semester type (spring/fall), holiday, football game-day, and academic year controls. Day of the week controls are included to address day-to-day fluctuations, while semester controls are included to adjust for activities that vary across the year such as fraternity recruitment. Moreover, football game-day controls are included to account for the increases in both alcohol offenses and rapes that college football games cause [@rees_college_2009; @lindo_college_2018].^[Information on football game dates and locations are found using sports-reference.com and espn.com. In total, 34 of the 37 universities in the sample that have football teams resulting in over 2000 football games, 89 of which coincide with a moratorium.] Lastly, holiday controls^[Holiday controls include indicators for Veterans Day, Thanksgiving, Labor Day, Halloween, and MLK Day. Christmas/New Years/July 4th are not included since no university's academic-calendar contains them.] are included since there may be less student activity on holidays and academic year controls are included due to differences between fraternity rules and guidelines between academic-years. Taken together, the corresponding interpretation of the parameter of interest, $\beta$, is the average difference in offense $Y_{u,t}$ on moratorium days relative to non-moratorium days, conditional on the expected differences between universities, days of the week, holidays, semesters, football game-days, and academic-years. 

I expand on the baseline model in Equation \ref{main_model} using several other specifications to allow for more flexibility in controlling for differences between universities' academic years. In particular, I progressively add both university-by-academic-year and university-by-academic-year-by-semester fixed effects to allow for several different comparisons within the model. The inclusion of university-by-academic year fixed effects allows for comparisons within university academic years while university-by-academic-year-by-semester fixed effects allows for comparisons within a semester during a university's academic year. While university-by-academic-year-by-semester fixed effects are most flexible, a large fraction (33%) of moratoriums span across multiple academic-year-semesters which leads to a small number of comparisons within each university-academic-year-semester. Moreover, as shown later in Section \ref{section:results}, including these fixed effects produces less conservative results than the inclusion of university-by-academic-year fixed effects. Because of this, the preferred specification utilizes university-by-academic-year fixed effects, although I show that the results are similar across all empirical approaches. Hence, unless otherwise noted, all analysis in this paper utilizes the preferred specification which utilizes university-by-academic-year fixed effects. 



## Identification Assumptions

In order for $\beta$ to be interpreted as a casual effect of fraternity moratoriums, there are three main assumptions that need to be satisfied: the timing of fraternity moratoriums are as-good-as-random, there are no changes in reporting/policing during a moratorium, and moratoriums have no lasting effects. 

To address the first assumption of as-good-as-random timing, I estimate a `multiple event' event study to identify any trends prior to a moratorium. Given that each university can experience multiple moratoriums and each moratorium is a different length, a staggered adoption event-study design is not appropriate. Therefore, to estimate the event study, I follow the guidelines outlined in @schmidheiny_event_2020; I generalize a classic dummy variable event study to accommodate multiple moratoriums within a university and classify the event-time (e.g., period 0) as the entire moratorium period. Therefore, period 0 represents all moratorium days within a university. 

Figures \ref{es_alc} and \ref{es_sex} show the results of the `multiple event' event study which demonstrate that there is little suggestive evidence that crime is already decreasing prior to a moratorium. Recall that the shaded area (period 0) represents an entire moratorium period while each lead and lag represents a 14-day period prior to or proceeding a moratorium (normalized by the 14-day period immediately proceeding a moratorium). 14-day periods are chosen in lieu of 7-day periods to allow for more precise point estimates. Five periods before and after are estimated, but only four are included as the fifth lead and lag are binned endpoints as described in @schmidheiny_event_2020. The errorbars represent 95% confidence intervals while the number of periods before/after the moratorium period are chosen to give approximately a median moratorium length of days (46) before and after the moratorium period. In each figure, there is little visual  evidence of a downward or upward trend prior to a moratorium. This is reinforced with statistically insignificant F-test showing that the three pre-periods are jointly zero at the five and 10 percent level.^[As a measure of robustness, an alternative event-study is estimated using 46-day periods before and after a moratorium in Figures \ref{es_alc_46_g} and \ref{es_sex_46_g}. Each of these figures fails to show evidence of a decreasing or increasing pre-period trend.] Moreover, the results of this analysis are intuitive; moratoriums are caused by triggering events in which typical behavior is taken "too far" and are usually enacted within three days following such event,^[This statistic is based on 13 of the 15 universities in which I have data on date of the triggering event.] thus giving little reason to expect anticipatory effects. In addition, according to an online repository of fraternity-related deaths from journalist Hank Nuwer, there are 19 universities that experienced a fraternity-related death but *did not* undergo a moratorium in the sample period which suggests that fraternity members may not expect a moratorium even when experiencing a particularly salient act of misconduct. Taken together, there is little suggestive evidence of a decreasing crime trend prior to a moratorium.

To test the second assumption of no changes in policing or reporting of offenses between moratorium and non-moratorium days, I conduct an indirect test that shows that there is no significant change between reporting behavior. In particular, I test whether the time of occurrence to the time of incident reported changes during moratorium days. This test is motivated by the notion that the amount of time from an occurrence to an official report may be due to factors such as police force staffing or the willingness of students to report. To perform this test, I construct the proportion of offenses that are reported with a lag on a given day for each offense.^[Only 32 of the 37 universities had data for the date occurred of their incidents. Hence, this test only reflects a subset of the sample.] An offense is defined as reported with a lag if the date the incident occurred is not equal to the date the offense was reported. 

Table \ref{reporting_table} shows that there is no significant change between the proportion of crimes reported with a lag during a moratorium. As a measure of robustness, I change the definition of a lag to reflect a difference of one, three, seven, and 14 days between the date occurred and date reported.^[Literature such as @sahay_silenced_2021 use a 3-day lag when applying this test.] Panel A shows that roughly 0.3% of alcohol offenses are reported with a one-day lag, and the change during a moratorium is insignificant from zero. Similarly, Panel B shows no difference in lagged reporting for sexual assault offenses. While sexual assaults have a higher proportion of reports that are reported with a lag (1.7%), the change during a moratorium is also insignificant from zero. 

For the final assumption, I conduct two series of analysis which are further discussed in Section \ref{section:results} to show that moratoriums have no lasting effects. Note that Equation \ref{main_model} implicitly assumes that student behavior changes only during moratoriums and that this behavior change does not persist over time. To address this, I supplement the preferred specification with a week lead and week lag to identify whether the effects of moratoriums instantaneously disappear once a moratorium is lifted. Moreover, I conduct an F-test on the 4 post periods in the `multiple event' event study and show that there is no significant post trend. These results further justify the use of already-treated universities (e.g., universities that have already experienced a moratorium) as a reasonable control group—a common critique of the difference-in-differences estimator with variation in treatment timing [@goodman-bacon_differences_2021]. Given that moratoriums show no lasting effects, an academic-calendar day without a moratorium is a good counterfactual for an academic-calendar day with a moratorium.


## Sample Challenges and Difference-in-Differences Literature

Several recent journal articles have found that using OLS in a two-way-fixed-effects (TWFE) difference-in-differences design can cause problematic issues with the coefficient estimates when there are heterogeneous treatment effects between groups over time [@de_chaisemartin_two-way_2020; @sun_estimating_2021; @athey_design-based_2022]. In particular, the parameter of interest (e.g., the coefficient on the treatment variable) is a weighted sum of average treatment effects where some of the weights may be negative. Additionally, negative weights occur when the two-way-fixed-effects estimator uses treated observations as controls [@goodman-bacon_differences_2021;@de_chaisemartin_two-way_2020;@borusyak_revisiting_2022]. While this paper's research design is not a typical TWFE design since treatment can occur multiple times and the preferred specification uses interacted fixed effects, there maintains a possibility that the negative weights issue could extend to the preferred model used in this paper due to the exclusion of never-treated units. 

In light of these potential issues, I conduct two series of analysis. First, Appendix \ref{section:twfe} analyzes a typical TWFE design in this setting using university and day-by-month-by-year fixed effects. I show that this design does not contain negative weights and that the coefficient estimates are consistent with the main results. Although this procedure is imperfect, the new estimators proposed by @callaway_differences_2021 and @de_chaisemartin_two-way_2020 are not suitable for this research design since the specification is not staggered, treatment units are treated multiple times, and fixed effects are interacted. Second, as discussed in Section \ref{section:results}, I include 14 never-treated universities to potentially reduce the occurrence of negative weights. The results in this analysis are consistent with the main findings. 


