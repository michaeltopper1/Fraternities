---
title: "Empirical Strategy"
output: pdf_document
linkcolor: blue
bibliography: "references.bib"
link-citations: yes

---

# Empirical Strategy \label{section:strategy}

## Empirical Approach \label{section:approach}

In order to estimate the average causal effect of fraternity moratoriums on alcohol and sexual assault offenses, I estimate the following baseline difference-in-differences specification using OLS:

\begin{equation}\label{main_model}
Y_{u,t} = \beta Moratorium_{u,t}  + \gamma_{u} + \lambda \mathbb{X}_{t} + \epsilon_{u,t}
\end{equation}

\noindent where $Y_{u,t}$ is an outcome of alcohol offenses or sexual assaults per-25000 enrolled students per academic-calendar day at university $u$ in time $t$. $Moratorium_{u,t}$ is an indicator variable equal to one when university $u$ is undergoing a moratorium at time $t$, $\gamma_u$ is a university-specific fixed effect, $\mathbb{X}_t$ is a vector of time-varying controls that are shared across universities, and $\epsilon_{u,t}$ is the error term. The standard errors are clustered by university to account for serial correlation within each university [@bertrand_how_2004]. In essence, Equation \ref{main_model} is comparing moratorium days to non-moratorium days within universities that have experienced, or will experience a moratorium while accounting for expected differences across universities and time.

Including university-specific fixed effects ($\gamma_u$) in the baseline model accounts for systematic differences between a university's police department, the corresponding student demographic they are are policing, and overall fixed differences in incident prevalence. For instance, university police departments may have systematic differences in the frequency of reporting due to the corresponding party-culture of their university or their own policing practices/resources, which in-turn, may lead to stronger or weaker enforcement of student drinking. Hence, including university-specific fixed effects ensures that moratorium days are compared to non-moratorium days while adjusting for these expected differences in universities. Moreover, $\mathbb{X}_{t}$ includes day of the week, semester type (spring/fall), holiday, football game-day, and academic-year controls. Day of the week controls are included to address day-to-day fluctuations, while semester controls are included to adjust for activities that vary across the year such as fraternity recruitment. Additionally, football game-day controls are included to account for the increases in both alcohol offenses and rapes that college football games cause [@rees_college_2009; @lindo_college_2018].^[Information on football game dates and locations are found using sports-reference.com and espn.com. In total, 34 of the 37 universities in the sample that have football teams resulting in over 2000 football games, 89 of which coincide with a moratorium.] Lastly, holiday controls^[Holiday controls include indicators for Veterans Day, Thanksgiving, Labor Day, Halloween, and MLK Day. Christmas/New Years/July 4th are not included since no university's academic-calendar contains them.] are included since there may be less student activity on holidays and academic year controls are included due to differences between fraternity rules and guidelines between academic-years. Taken together, the corresponding interpretation of the parameter of interest, $\beta$, is the average difference in offense $Y_{u,t}$ on moratorium days relative to non-moratorium days, conditional on the expected differences between universities, days of the week, holidays, semesters, football game-days, and academic-years. 

I expand on the baseline model in Equation \ref{main_model} using several other specifications to allow for more flexibility in controlling for differences between universities' academic years. In particular, I progressively add both university-by-academic-year and university-by-academic-year-by-semester fixed effects to allow for several different comparisons within the model. The inclusion of university-by-academic year fixed effects allow for comparisons within university academic years while university-by-academic-year-by-semester fixed effects allows for comparisons within a semester during a university's academic year. While university-by-academic-year-by-semester fixed effects are most flexible, a large fraction (33%) of moratoriums span across multiple academic-year-semesters which leads to a small number of comparisons within each university-academic-year-semester. Moreover, as shown later in Section \ref{section:mainresults}, including these fixed effects produces less conservative results than the inclusion of university-by-academic-year fixed effects. Because of this, the preferred specification utilizes university-by-academic-year fixed effects, although I show that the results are similar across all empirical approaches. Hence, unless otherwise noted, all analyses in this paper utilize the preferred specification which include university-by-academic-year fixed effects. 



## Identification Assumptions \label{section:assumptions}

To estimate Equation \ref{main_model} and interpret $\beta$ as a casual effect of fraternity moratoriums, there are three main assumptions that need to be satisfied: the timing of fraternity moratoriums is as-good-as-random, there are no changes in reporting/policing during a moratorium, and moratoriums have no lasting effects. 

To address the first assumption of as-good-as-random timing, I estimate a `multiple event' event study to identify any trends prior to a moratorium. Given that each university can experience multiple moratoriums and each moratorium is a different length, a staggered adoption event-study design is not appropriate. Therefore, to estimate the event study, I follow the guidelines outlined in @schmidheiny_event_2020; I generalize a classic dummy variable event study to accommodate multiple moratoriums within a university and classify the event-time (i.e., period 0) as the entire moratorium period. Therefore, period 0 represents all moratorium days within a university. 

Figures \ref{es_alc} and \ref{es_sex} show the results of the `multiple event' event study which demonstrate that there is little suggestive evidence that crime is already decreasing prior to a moratorium. Recall that the shaded area (period 0) represents an entire moratorium period while each lead and lag represents a 14-day period prior to, or proceeding a moratorium (normalized by the 14-day period immediately proceeding a moratorium). 14-day periods are chosen in lieu of 7-day periods to allow for more precise point estimates. Five periods before and after are estimated, but only four are included as the fifth lead and lag are binned endpoints as described in @schmidheiny_event_2020. The errorbars represent 95% confidence intervals while the number of periods before/after the moratorium period are chosen to give approximately a median moratorium length of days (46) before and after the moratorium period. In each figure, there is little visual  evidence of a downward or upward trend prior to a moratorium. This is reinforced with statistically insignificant F-test showing that the three pre-periods are jointly zero at the five and 10 percent level.^[As a measure of robustness, an alternative event-study is estimated using 46-day periods before and after a moratorium in Figures \ref{es_alc_46_g} and \ref{es_sex_46_g}. Each of these figures fails to show evidence of a decreasing or increasing pre-period trend.] Moreover, the results of this analysis are intuitive; moratoriums are caused by triggering events in which typical behavior is taken "too far" and are usually enacted within three days following such event,^[This statistic is based on 13 of the 15 universities in which I have data on date of the triggering event.] thus giving little reason to expect anticipatory effects. In addition, according to an online repository of fraternity-related deaths from journalist Hank Nuwer, there are 19 universities that experienced a fraternity-related death but *did not* undergo a moratorium in the sample period which suggests that fraternity members may not expect a moratorium even when experiencing a particularly salient act of misconduct. Taken together, there is little suggestive evidence of a decreasing crime trend prior to a moratorium.

To test the second assumption that moratoriums do not change policing or offense reporting, I conduct an indirect test which shows no significant change between reporting behavior. In particular, I test whether
there are significant differences between the time incidents occur and the time they are reported during a moratorium (i.e. *reporting lags*). This test is motivated by the notion that reporting lags may be due to factors such as police force staffing or the willingness of students to report. To perform this test, I construct the proportion of offenses that are reported with a lag on a given day for each offense.^[Only 32 of the 37 universities had data for the date occurred of their incidents. Hence, this test only reflects a subset of the sample.] An offense is defined as reported with a lag if the date the incident occurred is not equal to the date the offense was reported. 

Table \ref{reporting_table} shows that there is no significant change between the proportion of crimes reported with a lag during a moratorium. As a measure of robustness, I change the definition of a lag to reflect a difference of one, three, seven, and 14 days between the date occurred and date reported.^[Literature such as @sahay_silenced_2021 use a 3-day lag when applying this test.] Panel A shows that roughly 0.3% of alcohol offenses are reported with a one-day lag, and the change during a moratorium is insignificant. Similarly, Panel B shows no difference in lagged reporting for sexual assault offenses. While sexual assaults have a higher proportion of reports that are reported with a lag (1.7%), the change during a moratorium is also insignificant. 

For the final assumption, I conduct two series of analysis which are further discussed in Section \ref{section:longrun} to show that moratoriums have no lasting effects. Note that Equation \ref{main_model} implicitly assumes that student behavior changes only during moratoriums and that this behavior change does not persist over time. To address this, I supplement the preferred specification with a week lead and week lag to identify whether the effects of moratoriums instantaneously disappear once a moratorium is lifted. Moreover, I conduct an F-test on the 4 post periods in the `multiple event' event study and show that there is no significant post trend. These results further justify the use of already-treated universities (i.e., universities that have already experienced a moratorium) as a reasonable control groupâ€”a common critique of the difference-in-differences estimator with variation in treatment timing [@goodman-bacon_differences_2021]. Given that moratoriums show no lasting effects, an academic-calendar day without a moratorium is a good counterfactual for an academic-calendar day with a moratorium.


## Sample Challenges and Difference-in-Differences Literature \label{section:did}

Several recent journal articles have found that using OLS in a two-way-fixed-effects (TWFE) difference-in-differences design can cause issues with the coefficient estimates in the presence of heterogeneous treatment effects between groups over time [@de_chaisemartin_two-way_2020; @sun_estimating_2021; @athey_design-based_2022]. In particular, the coefficient on the explanatory variable for treatment is a weighted sum of average treatment effects where some of the weights may be negative. This negative weighting occurs when the two-way-fixed-effects estimator uses treated observations as controls [@goodman-bacon_differences_2021;@de_chaisemartin_two-way_2020;@borusyak_revisiting_2022]. While this paper's research design is not a typical TWFE design since treatment is not staggered, the treatment can occur multiple times, and the preferred specification uses interacted group and time fixed effects, there remains a possibility that the negative weights issue could extend to the preferred model used in this paper due to the exclusion of never-treated units. Since the new estimators proposed by @callaway_differences_2021 and @de_chaisemartin_two-way_2020 are not suitable for this experimental design, I conduct two different series of analyses which yield consistent results with my main findings: one that analyzes a typical TWFE design in this setting using university and day-by-month-by-year fixed effects (see Appendix \ref{section:twfe}), and another that includes 14 never-treated universities to potentially reduce the occurrence of negative weights (see Section \ref{section:results}). 



