---
title: "Empirical Strategy"
output: pdf_document
---

# Empirical Strategy \label{section:strategy}

## Baseline and Preferred Specifications

The goal of this paper is to identify the average causal effect of fraternity moratoriums on alcohol, drug, and sexual assault offenses across universities that experience moratoriums. In a naive analysis, this would amount to taking a difference of means for moratorium days and non-moratorium days across all universities for each offense. However, there are several issues with identifying such difference of means as a causal effect. First, university police departments each vary considerably in the frequency of reporting offenses. This is the result of differing policing tactics, the departments' available resources—such as number of officers per-student—and the overall composition of students at the university. For instance, a police department that oversees a university with a reputation for partying may police differently than a police department that rarely encounters college partying. Second, frequencies of offenses vary depending on the day of the week and the time of year. As an example, alcohol offenses most commonly occur on Fridays, Saturdays and Sundays, and fraternity recruitment is typically in the fall semester. A simple difference of means fails to account for each of these systematic differences between universities, days of the week, and semester. 

To circumvent these issues, I estimate the following baseline difference-in-differences specification using OLS:

\begin{equation}\label{main_model}
Y_{u,t} = \beta Moratorium_{u,t}  + \gamma_{u} + \lambda \mathbb{X}_{t} + \epsilon_{u,t}
\end{equation}

where $Y_{u,t}$ is an outcome of alcohol offenses or sexual assaults per-25000 enrolled students per academic-calendar day at university $u$ in time $t$. $Moratorium_{u,t}$ is an indicator variable equal to one when university $u$ is undergoing a moratorium at time $t$, $\gamma_u$ is a university-specific fixed effect, $\mathbb{X}_t$ is a vector of time-varying controls that are shared across universities, and $\epsilon_{u,t}$ is the error term. In essence, Equation \ref{main_model} is comparing moratorium days to non-moratorium days within universities that have experienced, or will experience a moratorium while accounting for expected differences across universities and time.

Including university-specific fixed effects ($\gamma_u$) in the baseline model accounts for systematic differences between a university's police department, the corresponding student demographic they are are policing, and overall fixed differences in incident prevalence. As stated above, a police department may have systematic differences in the frequency of reporting due to the corresponding demographic of the university or their own policing practices. For example, some police departments may enforce policies against underage drinking stricter than others. Hence, including university-specific fixed effects ensures that moratorium days are compared to non-moratorium days while adjusting for these expected differences in universities. Moreover, $\mathbb{X}_{t}$ includes day of the week, semester type (spring/fall), holiday, and academic year controls. Day of the week controls are included to address day-to-day fluctuations, while semester controls are included to adjust for activities that vary across the year such as fraternity recruitment and university football games. Lastly, holiday controls are included since there may be less student activity on holidays and academic year controls are included due to differences between fraternity rules and guidelines between academic years. Taken together, the corresponding interpretation of the parameter of interest, $\beta$, is the average difference in offense $Y_{u,t}$ on moratorium days relative to non-moratorium days, conditional on the expected differences between universities, days of the week, holidays, semesters, and academic years. 

The preferred specification slightly modifies Equation \ref{main_model}'s controls. In particular, I interact university-specific and academic-year fixed effects to allow more flexibility in controlling for differences between universities' academic years. For example, university regulations regarding fraternity recruitment or registration of social events may change by academic year. Hence, the preferred specification (see column (3) in Table \ref{results}) compares only university-specific academic calendar days with a moratorium to university-specific academic calendar days without a moratorium. A more data-intensive specification using university-by-academic-year-by-semester fixed effect is analyzed in Section \ref{section:results}. However, this specification is not preferred as its coefficient estimates are less conservative and a large fraction (33%) of moratoriums span across multiple academic-year-semesters. Unless otherwise noted, all analysis in this paper utilizes the preferred specification which includes the interaction of university and academic year fixed effects. 

## Identification Assumptions

In order for $\beta$ to be interpreted as a casual effect of fraternity moratoriums, there are four main assumptions that need to be satisfied. The first assumption is that the timing of a fraternity moratorium is as-good-as-random. There are several reasons why this may be plausible. First, fraternity moratoriums are the result of three types of triggering events: a fraternity-related death, a behavior violation, or a sexual assault (see Figure \ref{trigger_plot}). Fraternity-related deaths and behavior violations are the result of alcohol poisoning from binge drinking and hazing/rule violations respectively. Since fraternities commonly engage in binge drinking and hazing frequently [@desimone_fraternity_2007;@hechinger_true_2017], it is reasonable to assume that the timing of these extreme instances were coincidental. Similarly, studies have linked a higher prevalence of partying to fraternity members [@routon_impact_2014] which are linked to increased reports of sexual assault [@lindo_college_2018] and therefore a more salient occurrence is likely to have come by-chance rather than the result of unusual behavior. Second, it is common that the start of each moratorium coincides with its corresponding triggering event. Table \ref{reasons_table} shows a brief description of each triggering event in addition to the date of the triggering event and date of the enacted moratorium. In 14 of the 16 moratoriums in which the date of the triggering event is available, the moratorium is enacted within three days of the triggering event. While this is only a small subset of the sample due to data availability, this nonetheless provides evidence that students are unlikely changing their behavior in anticipation of a moratorium. Lastly, according to an online repository of fraternity-related deaths from journalist Hank Nuwer,^[See https://www.hanknuwer.com/hazing-deaths/.] there were 19 universities that experienced a fraternity-related death but *did not* undergo a moratorium in the sample period. Taken together, it is not clear that fraternity members or students expect a fraternity moratorium based on a triggering event.


The second assumption is that there are no changes in policing or reporting of offenses between moratorium and non-moratorium days. For instance, if university police reduced the number of on-duty officers during moratorium days in anticipation of less crime, the number of offenses reported in the Daily Crime Logs would be mechanically smaller because of changes in officers rather than the moratorium itself. Furthermore, students may have more (or less) inclination to report crimes such as sexual assaults if they act in response to the public pressure that moratoriums place on fraternities.  Since the Daily Crime Logs contain no information on number of on-duty officers or a student's affinity to report crimes, there is no direct way to test this assumption. However, as an indirect test, I analyze whether the time of occurrence to time of incident reported changes during moratorium days. This test is motivated by the notion that the amount of time from an occurrence to an official report may be due to factors such as police force staffing or the willingness of students to report. In particular, I construct the proportion of offenses that are reported with a lag on a given day for each offense.^[Only 33 of the 38 universities had data for the date occurred of their incidents. Hence, this test only reflects a subset of the sample.] An offense is defined as reported with a lag if the date the incident occurred is not equal to the date the offense was reported. Table \ref{reporting_table} shows the results of this test. In each column, I change the definition of a lag to reflect a difference of one, three, seven, and fourteen days between the date occurred and date reported.^[Literature such as @sahay_silenced_2021 use a 3-day lag when applying this test.] In each panel and column, the estimations show tight statistical zeros, therefore exhibiting no difference in the proportion of incidents reported with a lag. 

The third assumption stems from the difference-in-differences design of the model: common trends. While it is impossible to know the number of offenses that university's would have experienced in absence of a moratorium, a difference-in-differences model requires only that universities were experiencing similar trends prior to a moratorium. To test this assumption, I estimate a multiple-event event study following the guidelines outlined in @schmidheiny_event_2020: I generalize a classic dummy variable event study to accommodate multiple moratoriums within a university. Importantly, the event study is not staggered—the indicator for being within a moratorium contains the *entire* moratorium period. Hence, the leads and lags only represent periods where a moratorium is not in effect. This decision was made due to the differences in moratorium lengths—moratorium lengths can vary considerably across universities (see Table \ref{summary_stats}). Figures \ref{es_alc}, \ref{es_drug}, and \ref{es_sex} show the results of the multiple-event event study. The shaded area represents an entire moratorium period while each point estimate before and after represents a 14-day period prior to or proceeding a moratorium (normalized by the 14-day period immediately before a moratorium). 14-day periods are chosen in lieu of 7-day periods to allow for a more precise point estimate. Five periods before and after are estimated, but only four are included. The fifth lead and lag are binned endpoints as described in @schmidheiny_event_2020. The errorbars represent 95% confidence intervals while the event window (e.g., the number of periods before/after the moratorium period) was chosen to give approximately a median moratorium length of days (46) before and after the moratorium period. In each figure, there is no evidence of a downward or upward trend prior to a moratorium. This is reinforced with a joint F-test that the three pre-periods are zero are statistically insignificant at the 5 or 10 percent level. As a measure of robustness, an alternative event-study is estimated using 46-day periods before and after a moratorium in Figures \ref{es_alc_46_g}, \ref{es_drug_46_g}, and \ref{es_sex_46_g}. Each of these figures fails to show evidence of a decreasing or increasing pre-period trend. 

The fourth and final assumption is that moratoriums have no lasting effects. Equation \ref{main_model} implicitly assumes that student behavior changes only during moratoriums and that this behavior change does not persist over time. This is demonstrated through the fact that six universities in the sample experience more than one moratorium in the six-year period. If behavior truly changed, there would be no reason to enact multiple moratoriums. Moreover, in Section \ref{section:results}, I test this assumption by enriching the model with an indicator function for the week before and week after a moratorium. As discussed later, there is no evidence that there are persistent effects in the week following a moratorium. 





